{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "aa29655b",
      "metadata": {},
      "source": [
        "# Imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "3d3d4b1e",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Random Seed: 1343577804\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import numpy as np\n",
        "import cv2\n",
        "import matplotlib.pyplot as plt\n",
        "import itertools\n",
        "import torch_fidelity\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torchvision.datasets import ImageFolder\n",
        "from PIL import Image\n",
        "import torchvision.transforms as transforms\n",
        "import pandas as pd\n",
        "import random\n",
        "import torch.nn.functional as F\n",
        "import sys\n",
        "from tqdm import tqdm\n",
        "\n",
        "SEED = random.randrange(2**32 - 1)\n",
        "# SEED = 2853739981\n",
        "torch.manual_seed(SEED)\n",
        "torch.cuda.manual_seed_all(SEED)\n",
        "np.random.seed(SEED)\n",
        "random.seed(SEED)\n",
        "torch.backends.cudnn.deterministic = True\n",
        "torch.backends.cudnn.benchmark = False\n",
        "print(\"Random Seed:\", SEED)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b974f4bc",
      "metadata": {},
      "source": [
        "# Work"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0d800396",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Total parameters in CycleGAN model: 27.05 million\n"
          ]
        }
      ],
      "source": [
        "from models.generators import UNetGenerator, FastStyleGenerator, DefaultGenerator, ZhirongGenerator, BasicGenerator\n",
        "from models.discriminators import PatchDiscriminator, GithubDiscriminator, NLayerDiscriminator, RandomKaggleDiscriminator, PatchSpectralDiscriminator, DefaultDiscriminator\n",
        "\n",
        "\"\"\"\n",
        "Step 4. Initalize G and D\n",
        "\"\"\"\n",
        "G_AB = UNetGenerator()\n",
        "D_B = GithubDiscriminator()\n",
        "G_BA = BasicGenerator()\n",
        "D_A = GithubDiscriminator()\n",
        "\n",
        "## Total parameters in CycleGAN should be less than 60MB\n",
        "total_params = sum(p.numel() for p in G_AB.parameters()) + \\\n",
        "               sum(p.numel() for p in G_BA.parameters()) + \\\n",
        "               sum(p.numel() for p in D_A.parameters()) + \\\n",
        "               sum(p.numel() for p in D_B.parameters())\n",
        "\n",
        "\n",
        "def weights_init(m):\n",
        "    classname = m.__class__.__name__\n",
        "    if classname.find('Conv') != -1:\n",
        "        nn.init.xavier_uniform_(m.weight.data)\n",
        "    elif classname.find('BatchNorm') != -1:\n",
        "        nn.init.normal_(m.weight.data, 1.0, 0.02)\n",
        "        nn.init.constant_(m.bias.data, 0)\n",
        "    elif classname.find('Linear') != -1:\n",
        "        nn.init.xavier_uniform_(m.weight.data)\n",
        "        nn.init.constant_(m.bias.data, 0)\n",
        "\n",
        "# init weights\n",
        "# G_AB.apply(weights_init)\n",
        "# D_B.apply(weights_init)\n",
        "# G_BA.apply(weights_init)\n",
        "# D_A.apply(weights_init)\n",
        "\n",
        "\n",
        "total_params_million = total_params / (1024 * 1024)\n",
        "print(f'Total parameters in CycleGAN model: {total_params_million:.2f} million')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "1eafc098",
      "metadata": {},
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "Step 3. Define Loss\n",
        "\"\"\"\n",
        "from collections import defaultdict\n",
        "from loss_functions import GradientPreservationLoss, IdentityPreservationLoss, PatchNCELoss, HingeAdversarialLoss, EdgeConsistencyLoss, LineContinuityLoss\n",
        "\n",
        "# criterion_GAN = HingeAdversarialLoss()\n",
        "# criterion_GAN = nn.MSELoss()\n",
        "criterion_GAN = nn.BCELoss()\n",
        "criterion_cycle = nn.L1Loss()\n",
        "criterion_identity = IdentityPreservationLoss()\n",
        "# criterion_gradient = GradientPreservationLoss()\n",
        "# criterion_contrast = PatchNCELoss()\n",
        "# edge_loss = EdgeAwareLoss()\n",
        "\n",
        "\n",
        "def dynamic_loss_weighting(epoch, total_epochs):\n",
        "    # Early epochs: Focus on content preservation\n",
        "    d = defaultdict(float)\n",
        "    if epoch < total_epochs * 0.3:\n",
        "        d.update({\n",
        "            'identity': 5.0,\n",
        "            'gan': 1.0,\n",
        "            'cycle': 10.0,\n",
        "            'facial_component': 2.0,\n",
        "            'edge': 2.0,\n",
        "        })\n",
        "    elif epoch < total_epochs * 0.7:\n",
        "        d.update({\n",
        "            'identity': 2.0,\n",
        "            'gan': 1.0,\n",
        "            'cycle': 10.0,\n",
        "            'line': 2.0,\n",
        "            'facial_component': 5.0,\n",
        "        })\n",
        "    else:\n",
        "        d.update({\n",
        "            'identity': 1.0,\n",
        "            'gan': 1.5,\n",
        "            'cycle': 10.0,\n",
        "            'line': 3.0,\n",
        "            'facial_component': 5.0,\n",
        "        })\n",
        "    return d"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "0a855d14",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Current GPU: 0\n",
            "Current GPU name: NVIDIA GeForce RTX 4070 Ti\n"
          ]
        }
      ],
      "source": [
        "if torch.cuda.is_available():\n",
        "    print(f\"Current GPU: {torch.cuda.current_device()}\")\n",
        "    print(f\"Current GPU name: {torch.cuda.get_device_name(torch.cuda.current_device())}\")\n",
        "    DEVICE = torch.device(\"cuda\")\n",
        "    G_AB = G_AB.cuda()\n",
        "    D_B = D_B.cuda()\n",
        "    G_BA = G_BA.cuda()\n",
        "    D_A = D_A.cuda()\n",
        "    criterion_GAN = criterion_GAN.cuda()\n",
        "    criterion_cycle = criterion_cycle.cuda()\n",
        "    criterion_identity = criterion_identity.cuda()\n",
        "    TENSOR = torch.cuda.FloatTensor\n",
        "else:\n",
        "    print(\"PyTorch does not have access to GPU, falling back to CPU\")\n",
        "    TENSOR = torch.Tensor\n",
        "    DEVICE = torch.device(\"cpu\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4b2dcad7",
      "metadata": {},
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "Step 5. Configure Optimizers\n",
        "\"\"\"\n",
        "\n",
        "def get_lr_scheduler(optimizer, n_epochs=100, n_epochs_decay=50, lr_policy='linear', step_size=50, gamma=0.5, min_lr=1e-6, monitor='loss', patience=10):\n",
        "    if lr_policy == 'linear':\n",
        "        def lambda_rule(epoch):\n",
        "            # Keep constant for first n_epochs, then linearly decay to zero\n",
        "            lr_l = 1.0 - max(0, epoch - n_epochs) / float(n_epochs_decay + 1)\n",
        "            return lr_l\n",
        "        scheduler = torch.optim.lr_scheduler.LambdaLR(optimizer, lr_lambda=lambda_rule)\n",
        "\n",
        "    elif lr_policy == 'step':\n",
        "        # Decays the learning rate by gamma every step_size epochs\n",
        "        scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=step_size, gamma=gamma)\n",
        "\n",
        "    elif lr_policy == 'exponential':\n",
        "        # Exponentially decays the learning rate by gamma every epoch\n",
        "        scheduler = torch.optim.lr_scheduler.ExponentialLR(optimizer, gamma=gamma)\n",
        "\n",
        "    elif lr_policy == 'cosine':\n",
        "        # Cosine annealing from initial lr to min_lr over total epochs\n",
        "        scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=n_epochs + n_epochs_decay, eta_min=min_lr)\n",
        "\n",
        "    elif lr_policy == 'plateau':\n",
        "        # Reduces learning rate when a metric has stopped improving\n",
        "        scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min' if monitor == 'loss' else 'max', factor=gamma, patience=patience, min_lr=min_lr)\n",
        "\n",
        "    else:\n",
        "        raise NotImplementedError(f'learning rate policy {lr_policy} not implemented')\n",
        "\n",
        "    return scheduler\n",
        "\n",
        "# Optimizer setup\n",
        "optimizer_G = torch.optim.Adam(itertools.chain(G_AB.parameters(), G_BA.parameters()),\n",
        "                               lr=0.0002, betas=(0.5, 0.999))\n",
        "optimizer_D = torch.optim.Adam(itertools.chain(D_A.parameters(), D_B.parameters()),\n",
        "                               lr=0.0002, betas=(0.5, 0.999))\n",
        "\n",
        "# Learning rate schedulers\n",
        "scheduler_G = get_lr_scheduler(optimizer_G, lr_policy='cosine', min_lr=0.00004, n_epochs=300, n_epochs_decay=100)\n",
        "scheduler_D = get_lr_scheduler(optimizer_D, lr_policy='cosine', min_lr=0.00004, n_epochs=300, n_epochs_decay=100)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "c8d048e8",
      "metadata": {},
      "outputs": [],
      "source": [
        "from typing import Type, Optional, Literal\n",
        "\n",
        "def evaluate(\n",
        "    model: torch.nn.Module,\n",
        "    input_dir: str,\n",
        "    output_dir: str,\n",
        "    ref_dir: str,\n",
        "    batch_size: int,\n",
        "    generate_transforms: transforms.Compose,\n",
        "    dataloader: Optional[DataLoader] = None,\n",
        "    mode: Optional[Literal['A_B', 'B_A']] = None,\n",
        "    verbose: bool = False\n",
        ") -> float:\n",
        "\n",
        "    if input_dir is not None and dataloader is None:\n",
        "        if not os.path.exists(output_dir):\n",
        "            os.makedirs(output_dir)\n",
        "        files = [os.path.join(input_dir, name) for name in os.listdir(input_dir)]\n",
        "\n",
        "        def file_loader():\n",
        "            for i in range(0, len(files), batch_size):\n",
        "                imgs = [generate_transforms(Image.open(files[j])) for j in range(i, min(len(files), i + batch_size))]\n",
        "                yield torch.stack(imgs, 0).type(TENSOR), files[i:i + batch_size]\n",
        "\n",
        "        loader = file_loader()\n",
        "\n",
        "    elif dataloader is None:\n",
        "        raise ValueError(\"Either input_dir or dataloader must be provided.\")\n",
        "\n",
        "    elif dataloader and mode is None:\n",
        "        raise ValueError(\"Mode must be provided if dataloader is provided.\")\n",
        "\n",
        "    elif mode == 'A_B':\n",
        "        if not os.path.exists('evaluation_dumps/A_B'):\n",
        "            os.makedirs('evaluation_dumps/A_B')\n",
        "        output_dir = 'evaluation_dumps/A_B'\n",
        "    elif mode == 'B_A':\n",
        "        if not os.path.exists('evaluation_dumps/B_A'):\n",
        "            os.makedirs('evaluation_dumps/B_A')\n",
        "        output_dir = 'evaluation_dumps/B_A'\n",
        "\n",
        "    model.eval()\n",
        "    ############## ORIGINAL CODE ##############\n",
        "    # for i in range(0, len(files), batch_size):\n",
        "    #     # Read and transform images\n",
        "    #     imgs = [generate_transforms(Image.open(files[j])) for j in range(i, min(len(files), i + batch_size))]\n",
        "    #     imgs = torch.stack(imgs, 0).type(TENSOR)\n",
        "\n",
        "    #     # Generate images\n",
        "    #     fake_imgs = model(imgs).detach().cpu()\n",
        "\n",
        "    #     # Save generated images\n",
        "    #     for j in range(fake_imgs.size(0)):\n",
        "    #         img = fake_imgs[j].squeeze().permute(1, 2, 0).numpy()\n",
        "    #         img = (img - np.min(img)) * 255 / (np.max(img) - np.min(img))\n",
        "    #         img = transforms.ToPILImage()(img.astype(np.uint8))\n",
        "    #         _, name = os.path.split(files[i + j])\n",
        "    #         img.save(os.path.join(output_dir, name))\n",
        "    ###########################################\n",
        "\n",
        "    ############### WITH DATALOADER SUPPORT ###############\n",
        "    with torch.no_grad():\n",
        "        for i, batch in enumerate(dataloader if dataloader else loader):\n",
        "            if isinstance(batch, (list, tuple)) and len(batch) == 4: # the only time that its a dataloader is when both A and B are \"zipped\"\n",
        "                imgs = (batch[0] if mode == 'A_B' else batch[1]).type(TENSOR)\n",
        "                filenames = batch[2] if mode == 'A_B' else batch[3]\n",
        "            else: # otherwise, its a single folder\n",
        "                imgs, filenames = batch\n",
        "\n",
        "            # Generate images\n",
        "            fake_imgs = model(imgs).detach().cpu()\n",
        "\n",
        "            # Save generated images\n",
        "            for img, name in zip(fake_imgs, filenames):\n",
        "                img = img.squeeze().permute(1, 2, 0).numpy()\n",
        "                img = (img - np.min(img)) * 255 / (np.max(img) - np.min(img))\n",
        "                img = transforms.ToPILImage()(img.astype(np.uint8))\n",
        "                img.save(os.path.join(output_dir, os.path.basename(name)))\n",
        "        torch.cuda.empty_cache()\n",
        "    ######################################################\n",
        "\n",
        "\n",
        "    # Compute metrics\n",
        "    metrics: dict[str, float] = torch_fidelity.calculate_metrics(\n",
        "        input1=output_dir,\n",
        "        input2=ref_dir if dataloader is None else dataloader.dataset.get_partial_dataset('B' if mode == 'A_B' else 'A'),\n",
        "        cuda=True,\n",
        "        fid=True,\n",
        "        isc=True,\n",
        "        verbose=verbose\n",
        "    )\n",
        "\n",
        "    fid_score: float = metrics[\"frechet_inception_distance\"]\n",
        "    is_score: float = metrics[\"inception_score_mean\"]\n",
        "\n",
        "    del imgs; del fake_imgs\n",
        "    if is_score > 0:\n",
        "        gms: float = np.sqrt(fid_score / is_score)\n",
        "        print(\"Geometric Mean Score:\", gms)\n",
        "        return gms, fid_score, is_score\n",
        "    else:\n",
        "        print(\"IS is 0, GMS cannot be computed!\")\n",
        "        return 0, 0, 0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2479d552",
      "metadata": {},
      "outputs": [],
      "source": [
        "from typing import Callable, Literal\n",
        "\n",
        "\n",
        "def run_one_epoch(\n",
        "    G_AB: nn.Module,\n",
        "    G_BA: nn.Module,\n",
        "    D_A: nn.Module,\n",
        "    D_B: nn.Module,\n",
        "    state: Literal[\"train\", \"eval\"],\n",
        "    dataloader: DataLoader,\n",
        "    criterion_identity: Callable,\n",
        "    criterion_GAN: Callable,\n",
        "    criterion_cycle: Callable,\n",
        "    optimizer_G: torch.optim.Optimizer,\n",
        "    optimizer_D: torch.optim.Optimizer,\n",
        "    # optimizer_D_A: torch.optim.Optimizer,\n",
        "    # optimizer_D_B: torch.optim.Optimizer,\n",
        "    lambdas: dict[str, float],\n",
        ") -> dict[str, float]:\n",
        "\n",
        "    G_AB.train(), G_BA.train()\n",
        "\n",
        "    running_losses = {\n",
        "        \"G\": 0.0, \"D_A\": 0.0, \"D_B\": 0.0,\n",
        "        \"identity\": 0.0, \"gan\": 0.0, \"cycle\": 0.0, \"line\": 0.0\n",
        "    }\n",
        "\n",
        "    with tqdm(dataloader, unit=\"batch\", desc=\"Training\" if state == \"train\" else \"Validation\") as tepoch:\n",
        "        for real_A, real_B, *_ in tepoch:\n",
        "            optimizer_G.zero_grad()\n",
        "            D_A.eval(), D_B.eval()\n",
        "            with torch.amp.autocast(DEVICE.type, enabled=USEAUTOCAST):\n",
        "                real_A, real_B = real_A.to(DEVICE, non_blocking=True), real_B.to(DEVICE, non_blocking=True)\n",
        "\n",
        "                # Train Generators\n",
        "                fake_B = G_AB(real_A)\n",
        "                fake_A = G_BA(real_B)\n",
        "\n",
        "                # Identity loss\n",
        "                loss_id_A = criterion_identity(G_BA(real_A), real_A)\n",
        "                loss_id_B = criterion_identity(G_AB(real_B), real_B)\n",
        "                loss_identity = (loss_id_A + loss_id_B) / 2\n",
        "\n",
        "                # GAN loss - trying to fool D\n",
        "                fake_B_pred = D_B(fake_B)\n",
        "                fake_A_pred = D_A(fake_A)\n",
        "\n",
        "                # because discriminator outputs are supposed to be identical no matter where, I can reuse\n",
        "                valid, fake = torch.ones_like(fake_A_pred, requires_grad=False), torch.zeros_like(fake_A_pred, requires_grad=False)\n",
        "\n",
        "                loss_GAN_AB = criterion_GAN(fake_B_pred, valid)\n",
        "                loss_GAN_BA = criterion_GAN(fake_A_pred, valid)\n",
        "                loss_GAN = (loss_GAN_AB + loss_GAN_BA) / 2\n",
        "\n",
        "                # Cycle loss\n",
        "                loss_cycle_A = criterion_cycle(G_BA(fake_B), real_A)\n",
        "                loss_cycle_B = criterion_cycle(G_AB(fake_A), real_B)\n",
        "                loss_cycle = (loss_cycle_A + loss_cycle_B) / 2\n",
        "\n",
        "                # # Line continuity loss\n",
        "                # loss_line_A = line_loss(fake_A, real_A)\n",
        "                # loss_line_B = line_loss(fake_B, real_B)\n",
        "                # loss_line = (loss_line_A + loss_line_B) / 2\n",
        "\n",
        "            # Total generator loss\n",
        "            loss_G = lambdas['identity'] * loss_identity + lambdas['gan'] * loss_GAN + lambdas['cycle'] * loss_cycle\n",
        "            loss_G.backward()\n",
        "            optimizer_G.step()\n",
        "\n",
        "\n",
        "\n",
        "            # Train Discriminators\n",
        "            optimizer_D.zero_grad()\n",
        "            D_A.train(), D_B.train()\n",
        "            with torch.amp.autocast(DEVICE.type, enabled=USEAUTOCAST):\n",
        "                real_A_pred = D_A(real_A)\n",
        "                fake_A_pred = D_A(fake_A.detach())\n",
        "                loss_real_A = criterion_GAN(real_A_pred, valid)\n",
        "                loss_fake_A = criterion_GAN(fake_A_pred, fake)\n",
        "                loss_D_A = (loss_real_A + loss_fake_A) / 2\n",
        "                \n",
        "                real_B_pred = D_B(real_B)\n",
        "                fake_B_pred = D_B(fake_B.detach())\n",
        "                loss_real_B = criterion_GAN(real_B_pred, valid)\n",
        "                loss_fake_B = criterion_GAN(fake_B_pred, fake)\n",
        "                loss_D_B = (loss_real_B + loss_fake_B) / 2\n",
        "                \n",
        "                loss_D = (loss_D_A + loss_D_B)\n",
        "            loss_D.backward()\n",
        "            optimizer_D.step()\n",
        "\n",
        "            # Accumulate losses\n",
        "            running_losses[\"G\"] += loss_G.item()\n",
        "            running_losses[\"D_A\"] += loss_D_A.item()\n",
        "            running_losses[\"D_B\"] += loss_D_B.item()\n",
        "            running_losses[\"identity\"] += loss_identity.item()\n",
        "            running_losses[\"gan\"] += loss_GAN.item()\n",
        "            running_losses[\"cycle\"] += loss_cycle.item()\n",
        "            # running_losses[\"line\"] += loss_line.item()\n",
        "\n",
        "    print(f'[G loss: {loss_G.item()} | identity: {loss_identity.item()} GAN: {loss_GAN.item()} cycle: {loss_cycle.item()}]')\n",
        "    print(f'[D loss: {loss_D.item()} | D_A: {loss_D_A.item()} D_B: {loss_D_B.item()}]')\n",
        "    # Average the losses over the dataset\n",
        "    return {k: v / len(dataloader) for k, v in running_losses.items()}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "6e619ac7",
      "metadata": {},
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "Step 6. DataLoader\n",
        "\"\"\"\n",
        "from CustomImageDataset import CustomImageDataset as ImageDataset\n",
        "# data_dir = '/kaggle/input/group-project/image_image_translation'\n",
        "data_dir = ''\n",
        "\n",
        "image_size = (256, 256)\n",
        "transforms_ = transforms.Compose([\n",
        "    transforms.Resize(image_size),\n",
        "    # transforms.RandomHorizontalFlip(p=0.5),  # 50% chance of flipping\n",
        "    # transforms.RandomAffine(degrees=5, translate=(0.1,0.1)),  # Position variation\n",
        "    # transforms.RandomPerspective(0.2, 0.3),  # Perspective distortion\n",
        "    # transforms.RandomPosterize(bits=5, p=0.3),  # Posterize\n",
        "    transforms.RandomApply([transforms.GaussianBlur(kernel_size=3, sigma=(0.1, 5.0))], 0.3),\n",
        "    # transforms.ColorJitter(brightness=0.4, contrast=0.2, saturation=0.2, hue=0.05),\n",
        "    # transforms.RandomErasing(p=0.1, scale=(0.05, 0.1)),  # Erase a part of the image\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
        "])\n",
        "\n",
        "evaluation_transforms = transforms.Compose([\n",
        "    transforms.Resize(image_size),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
        "])\n",
        "\n",
        "\n",
        "loader_params = {\n",
        "    \"batch_size\": 4,\n",
        "    \"num_workers\": 2,\n",
        "    \"pin_memory\": True,\n",
        "    \"shuffle\": True,\n",
        "    # \"prefetch_factor\": 2,\n",
        "    \"persistent_workers\": True\n",
        "}\n",
        "\n",
        "\n",
        "trainloader = DataLoader(\n",
        "    ImageDataset(data_dir, mode='train', transform=transforms_),\n",
        "    **loader_params\n",
        ")\n",
        "\n",
        "loader_params[\"shuffle\"] = False\n",
        "validloader = DataLoader(\n",
        "    ImageDataset(data_dir, mode='valid', transform=transforms_),\n",
        "    **loader_params\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "409ea73f-b928-4e03-bc93-65204998136c",
      "metadata": {
        "id": "409ea73f-b928-4e03-bc93-65204998136c",
        "trusted": true
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[Epoch 1/300]\n"
          ]
        },
        {
          "ename": "NameError",
          "evalue": "name 'run_one_epoch' is not defined",
          "output_type": "error",
          "traceback": [
            "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
            "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[7]\u001b[39m\u001b[32m, line 19\u001b[39m\n\u001b[32m     17\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[32m1\u001b[39m, n_epochs+\u001b[32m1\u001b[39m):\n\u001b[32m     18\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m'\u001b[39m\u001b[33m[Epoch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mn_epochs\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m]\u001b[39m\u001b[33m'\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m19\u001b[39m     epoch_loss = \u001b[43mrun_one_epoch\u001b[49m(\n\u001b[32m     20\u001b[39m         G_AB, G_BA, D_A, D_B, \u001b[33m\"\u001b[39m\u001b[33mtrain\u001b[39m\u001b[33m\"\u001b[39m, trainloader,\n\u001b[32m     21\u001b[39m         criterion_identity, criterion_GAN, criterion_cycle,\n\u001b[32m     22\u001b[39m         optimizer_G, optimizer_D,\n\u001b[32m     23\u001b[39m         dynamic_loss_weighting(epoch, n_epochs)\n\u001b[32m     24\u001b[39m     )\n\u001b[32m     26\u001b[39m     loss_history.append(epoch_loss)\n\u001b[32m     28\u001b[39m     \u001b[38;5;66;03m# validation\u001b[39;00m\n",
            "\u001b[31mNameError\u001b[39m: name 'run_one_epoch' is not defined"
          ]
        }
      ],
      "source": [
        "\"\"\"\n",
        "Step 7. Training\n",
        "\"\"\"\n",
        "n_epochs = 300\n",
        "EVALUATION_INTERVAL = 15\n",
        "evaluation_scaling = 1\n",
        "patience = 0\n",
        "USEAUTOCAST = False\n",
        "loss_history = []\n",
        "best_metrics = {\n",
        "    'AVG': float('inf'),\n",
        "    'AB': {'GMS': float('inf'), 'FID': float('inf'), 'IS': float('inf')},\n",
        "    'BA': {'GMS': float('inf'), 'FID': float('inf'), 'IS': float('inf')},\n",
        "    'BEST_AB': {'GMS': float('inf'), 'FID': float('inf'), 'IS': float('inf'), 'epoch': float('inf')},\n",
        "    'BEST_BA': {'GMS': float('inf'), 'FID': float('inf'), 'IS': float('inf'), 'epoch': float('inf')}\n",
        "}\n",
        "for epoch in range(1, n_epochs+1):\n",
        "    print(f'[Epoch {epoch}/{n_epochs}]')\n",
        "    epoch_loss = run_one_epoch(\n",
        "        G_AB, G_BA, D_A, D_B, \"train\", trainloader,\n",
        "        criterion_identity, criterion_GAN, criterion_cycle,\n",
        "        optimizer_G, optimizer_D,\n",
        "        dynamic_loss_weighting(epoch, n_epochs)\n",
        "    )\n",
        "\n",
        "    loss_history.append(epoch_loss)\n",
        "\n",
        "    # validation\n",
        "    if epoch % int(EVALUATION_INTERVAL * evaluation_scaling) == 0:\n",
        "        metrics = best_metrics.copy()\n",
        "        metrics['AB']['GMS'], metrics['AB']['FID'], metrics['AB']['IS'] = evaluate(G_AB, \n",
        "                                                                'NO_INPUT_DIR',\n",
        "                                                                'EVAL_DUMP',\n",
        "                                                                'NO_REF_DIR',\n",
        "                                                                loader_params['batch_size'],\n",
        "                                                                evaluation_transforms,\n",
        "                                                                validloader,\n",
        "                                                                'A_B',\n",
        "                                                                verbose=True)\n",
        "        metrics['BA']['GMS'], metrics['BA']['FID'], metrics['BA']['IS'] = evaluate(G_BA, \n",
        "                                                                'NO_INPUT_DIR',\n",
        "                                                                'EVAL_DUMP',\n",
        "                                                                'NO_REF_DIR',\n",
        "                                                                loader_params['batch_size'],\n",
        "                                                                evaluation_transforms,\n",
        "                                                                validloader,\n",
        "                                                                'B_A')\n",
        "\n",
        "        metricsString_AB = \" \".join([f\"{k}={v:4f}\" for k, v in metrics['AB'].items()])\n",
        "        metricsString_BA = \" \".join([f\"{k}={v:4f}\" for k, v in metrics['BA'].items()])\n",
        "        metrics['AVG'] = (metrics['AB']['GMS'] + metrics['BA']['GMS']) / 2\n",
        "\n",
        "        print(\"A->B |\", metricsString_AB)\n",
        "        print(\"B->A |\", metricsString_BA)\n",
        "        print(f\"Average GMS: {metrics['AVG']}, Best: {best_metrics['AVG']}\")\n",
        "\n",
        "        if metrics['AVG'] > best_metrics['AVG'] * 1.05:\n",
        "            print(\"ðŸ’©ðŸš½ Getting Cooked!\")\n",
        "\n",
        "        elif metrics['AVG'] < best_metrics['AVG']:\n",
        "            best_metrics['AB'] = metrics['AB'].copy()\n",
        "            best_metrics['BA'] = metrics['BA'].copy()\n",
        "            best_metrics['AVG'] = metrics['AVG']\n",
        "            patience = 0\n",
        "            print(\"(à¸‡ ðŸ”¥ ï¾› ðŸ”¥ )à¸‡ CooKING! ðŸš€\")\n",
        "            # Save model checkpoints  \n",
        "            torch.save(G_AB.state_dict(), f'checkpoints/G_AB_{epoch}.pth')\n",
        "            torch.save(D_A.state_dict(), f'checkpoints/D_A_{epoch}.pth')\n",
        "            torch.save(G_BA.state_dict(), f'checkpoints/G_BA_{epoch}.pth')\n",
        "            torch.save(D_B.state_dict(), f'checkpoints/D_B_{epoch}.pth')\n",
        "        else:\n",
        "            patience += 1\n",
        "\n",
        "        if patience >= 3:  # Reduce evaluation frequency\n",
        "            print(\"I'm losing my patience! Best GMS:\", best_metrics['AVG'])\n",
        "            evaluation_interval = int(1.5 * evaluation_scaling)\n",
        "        \n",
        "\n",
        "        if metrics['AB']['GMS'] < best_metrics['BEST_AB']['GMS']:\n",
        "            best_metrics['BEST_AB'] = metrics['AB'].copy()\n",
        "            best_metrics['BEST_AB']['epoch'] = epoch\n",
        "            torch.save(G_AB.state_dict(), f'checkpoints_best/G_AB.pth')\n",
        "            print('Best A->B model saved. Epoch:', epoch)\n",
        "\n",
        "        if metrics['BA']['GMS'] < best_metrics['BEST_BA']['GMS']:\n",
        "            best_metrics['BEST_BA'] = metrics['BA'].copy()\n",
        "            best_metrics['BEST_BA']['epoch'] = epoch\n",
        "            torch.save(G_BA.state_dict(), f'checkpoints_best/G_BA.pth')\n",
        "            print('Best B->A model saved. Epoch:', epoch)\n",
        "\n",
        "torch.save(G_AB.state_dict(), f'checkpoints/G_AB_{n_epochs}.pth')\n",
        "torch.save(D_A.state_dict(), f'checkpoints/D_A_{n_epochs}.pth')\n",
        "torch.save(G_BA.state_dict(), f'checkpoints/G_BA_{n_epochs}.pth')\n",
        "torch.save(D_B.state_dict(), f'checkpoints/D_B_{n_epochs}.pth')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "52cb0fb1",
      "metadata": {},
      "source": [
        "# Output Results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3d70abd1",
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "import pickle\n",
        "from datetime import datetime\n",
        "def plot_losses(loss_history):\n",
        "    formatted_loss_history = {k: [epoch_losses[k] for epoch_losses in loss_history] for k in loss_history[0]}\n",
        "    timestamp = datetime.now().strftime('%Y%m%d_%H%M')\n",
        "\n",
        "    with open(os.path.join(f\"loss_log-{timestamp}.pkl\"), 'wb') as f:\n",
        "        pickle.dump(formatted_loss_history, f)\n",
        "\n",
        "    plt.figure(figsize=(10, 6))\n",
        "    for loss_name, values in formatted_loss_history.items():\n",
        "        plt.plot(values, label=loss_name)\n",
        "\n",
        "    plt.xlabel('Epochs')\n",
        "    plt.ylabel('Loss')\n",
        "    plt.title('Training Losses Over Epochs')\n",
        "    plt.legend()\n",
        "    plt.grid(True)\n",
        "    plt.savefig(f\"loss_plot-{timestamp}.png\")\n",
        "    plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "84d6ed27",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Processing latest model...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Creating feature extractor \"inception-v3-compat\" with features ['logits_unbiased', '2048']\n",
            "Extracting features from input1\n",
            "Looking for samples non-recursivelty in \"../Cartoon_images\" with extensions png,jpg,jpeg\n",
            "Found 1000 samples, some are lossy-compressed - this may affect metrics\n",
            "c:\\Users\\a\\Desktop\\cs424-project\\venv313\\Lib\\site-packages\\torch_fidelity\\datasets.py:16: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
            "  img = torch.ByteTensor(torch.ByteStorage.from_buffer(img.tobytes())).view(height, width, 3)\n",
            "Processing samples                                                          \n",
            "Extracting features from input2\n",
            "Looking for samples non-recursivelty in \"./VAE_generation_Cartoon/test\" with extensions png,jpg,jpeg\n",
            "Found 1000 samples, some are lossy-compressed - this may affect metrics\n",
            "Processing samples                                                          \n",
            "Inception Score: 2.136652938583595 Â± 0.15139583515735827\n",
            "Frechet Inception Distance: 41.89671612520374\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Geometric Mean Score: 4.428156893062493\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Creating feature extractor \"inception-v3-compat\" with features ['logits_unbiased', '2048']\n",
            "Extracting features from input1\n",
            "Looking for samples non-recursivelty in \"../Raw_images\" with extensions png,jpg,jpeg\n",
            "Found 1000 samples, some are lossy-compressed - this may affect metrics\n",
            "Processing samples                                                          \n",
            "Extracting features from input2\n",
            "Looking for samples non-recursivelty in \"./VAE_generation/test\" with extensions png,jpg,jpeg\n",
            "Found 1000 samples, some are lossy-compressed - this may affect metrics\n",
            "Processing samples                                                          \n",
            "Inception Score: 3.737211246187865 Â± 0.2566777612262926\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Geometric Mean Score: 3.9784483740348797\n",
            "CSV saved to 300-4_2033-C[4_4282-41_8967-2_1367]-R[3_9784-59_1528-3_7372].csv\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Frechet Inception Distance: 59.15277193971801\n"
          ]
        }
      ],
      "source": [
        "# plot_losses(loss_history)\n",
        "\n",
        "# Parameters\n",
        "batch_size = loader_params[\"batch_size\"]\n",
        "\n",
        "# data_dir = '/kaggle/input/group-project/image_image_translation'\n",
        "data_dir = '.'\n",
        "\n",
        "\n",
        "def format_score(score):\n",
        "    return \"{:.4f}\".format(score).replace(\".\", \"_\")\n",
        "\n",
        "def report_score(s_value_1, fid_1, is_1, s_value_2, fid_2, is_2, epoch=None):\n",
        "    s_value = np.round((s_value_1+s_value_2)/2, 5)\n",
        "    df = pd.DataFrame({'id': [1], 'label': [s_value]})\n",
        "    filename = f\"{format_score(s_value)}-C[{format_score(s_value_1)}-{format_score(fid_1)}-{format_score(is_1)}]-R[{format_score(s_value_2)}-{format_score(fid_2)}-{format_score(is_2)}]\"\n",
        "    if epoch: filename = f\"{epoch}-{filename}\"\n",
        "    csv_path = filename+\".csv\"\n",
        "    df.to_csv(csv_path, index=False)\n",
        "\n",
        "    print(f\"CSV saved to {csv_path}\")\n",
        "\n",
        "\n",
        "\n",
        "# Process latest model\n",
        "print(\"Processing latest model...\")\n",
        "G_AB.load_state_dict(torch.load(f'curbest/with256/G_AB.pth'))\n",
        "G_BA.load_state_dict(torch.load(f'curbest/with256/G_BA.pth'))\n",
        "s_value_1, fid_1, is_1 = evaluate(\n",
        "    model=G_AB,\n",
        "    input_dir=os.path.join(data_dir, 'VAE_generation/test'),\n",
        "    output_dir='../Cartoon_images',\n",
        "    ref_dir=f\"{data_dir}/VAE_generation_Cartoon/test\",\n",
        "    batch_size=batch_size,\n",
        "    generate_transforms=evaluation_transforms,\n",
        "    verbose=True\n",
        ")\n",
        "s_value_2, fid_2, is_2 = evaluate(\n",
        "    model=G_BA,\n",
        "    input_dir=os.path.join(data_dir, 'VAE_generation_Cartoon/test'),\n",
        "    output_dir='../Raw_images',\n",
        "    ref_dir=f\"{data_dir}/VAE_generation/test\",\n",
        "    batch_size=batch_size,\n",
        "    generate_transforms=evaluation_transforms,\n",
        "    verbose=True\n",
        ")\n",
        "report_score(s_value_1, fid_1, is_1, s_value_2, fid_2, is_2, epoch=n_epochs)\n",
        "print()\n",
        "\n",
        "# Process best for each side\n",
        "# try:\n",
        "#     print(\"Processing best sides...\")\n",
        "#     G_AB.load_state_dict(torch.load('checkpoints_best/G_AB.pth'))\n",
        "#     G_BA.load_state_dict(torch.load('checkpoints_best/G_BA.pth'))\n",
        "#     s_value_1, fid_1, is_1 = evaluate(\n",
        "#         model=G_AB,\n",
        "#         input_dir=os.path.join(data_dir, 'VAE_generation/test'),\n",
        "#         output_dir='../Cartoon_images',\n",
        "#         ref_dir=f\"{data_dir}/VAE_generation_Cartoon/test\",\n",
        "#         batch_size=batch_size,\n",
        "#         generate_transforms=evaluation_transforms,\n",
        "#         verbose=True\n",
        "#     )\n",
        "#     s_value_2, fid_2, is_2 = evaluate(\n",
        "#         model=G_BA,\n",
        "#         input_dir=os.path.join(data_dir, 'VAE_generation_Cartoon/test'),\n",
        "#         output_dir='../Raw_images',\n",
        "#         ref_dir=f\"{data_dir}/VAE_generation/test\",\n",
        "#         batch_size=batch_size,\n",
        "#         generate_transforms=evaluation_transforms,\n",
        "#         verbose=True\n",
        "#     )\n",
        "#     report_score(s_value_1, fid_1, is_1, s_value_2, fid_2, is_2)\n",
        "# except:\n",
        "#     print(\"Failed to process best sides.\")\n",
        "# finally:\n",
        "#     print()\n",
        "\n",
        "# print(\"Processing saved model checkpoints...\")\n",
        "# checkpoints_dir = 'checkpoints'\n",
        "# generator_weights = defaultdict(lambda: [None, None])\n",
        "# for filename in os.listdir(checkpoints_dir):\n",
        "#     model_type, domain, epoch_num = filename[:-4].split('_')\n",
        "#     if model_type == 'G':\n",
        "#         generator_weights[epoch_num][0 if domain == 'AB' else 1] = os.path.join(checkpoints_dir, filename) \n",
        "\n",
        "# for epoch_num, (G_AB_path, G_BA_path) in generator_weights.items():\n",
        "#     # Process saved model\n",
        "#     G_AB.load_state_dict(torch.load(G_AB_path))\n",
        "#     G_BA.load_state_dict(torch.load(G_BA_path))\n",
        "#     print(f\"Evaluating epoch {epoch_num}\")\n",
        "\n",
        "#     print(\"Metrics for A -> B:\", end=\" \")\n",
        "#     # Raw to Cartoon\n",
        "#     s_value_1, fid_1, is_1 = evaluate(\n",
        "#         model=G_AB,\n",
        "#         input_dir=os.path.join(data_dir, 'VAE_generation/test'),\n",
        "#         output_dir='../Cartoon_images',\n",
        "#         ref_dir=f\"{data_dir}/VAE_generation_Cartoon/test\",\n",
        "#         batch_size=batch_size,\n",
        "#         generate_transforms=evaluation_transforms\n",
        "#     )\n",
        "\n",
        "#     # Cartoon to Raw\n",
        "#     s_value_2, fid_2, is_2 = evaluate(\n",
        "#         model=G_BA,\n",
        "#         input_dir=os.path.join(data_dir, 'VAE_generation_Cartoon/test'),\n",
        "#         output_dir='../Raw_images',\n",
        "#         ref_dir=f\"{data_dir}/VAE_generation/test\",\n",
        "#         batch_size=batch_size,\n",
        "#         generate_transforms=evaluation_transforms\n",
        "#     )\n",
        "\n",
        "#     report_score(s_value_1, fid_1, is_1, s_value_2, fid_2, is_2, epoch=epoch_num)\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6d3ad489",
      "metadata": {},
      "source": [
        "# Clean Up"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2ae7c7ab",
      "metadata": {},
      "outputs": [],
      "source": [
        "# import gc\n",
        "# del G_AB, G_BA, D_A, D_B\n",
        "\n",
        "# gc.collect()\n",
        "# with torch.no_grad():\n",
        "#     torch.cuda.empty_cache()"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kaggle": {
      "accelerator": "gpu",
      "dataSources": [
        {
          "datasetId": 6717840,
          "sourceId": 10819976,
          "sourceType": "datasetVersion"
        }
      ],
      "dockerImageVersionId": 30887,
      "isGpuEnabled": true,
      "isInternetEnabled": true,
      "language": "python",
      "sourceType": "notebook"
    },
    "kernelspec": {
      "display_name": "venv313",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.1"
    },
    "papermill": {
      "default_parameters": {},
      "duration": 9291.681683,
      "end_time": "2021-08-29T17:46:57.805996",
      "environment_variables": {},
      "exception": null,
      "input_path": "__notebook__.ipynb",
      "output_path": "__notebook__.ipynb",
      "parameters": {},
      "start_time": "2021-08-29T15:12:06.124313",
      "version": "2.3.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
