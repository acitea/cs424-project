{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "We3Y3X_hxaDx",
      "metadata": {
        "id": "We3Y3X_hxaDx"
      },
      "source": [
        "## 1. Import the dependencies"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "df4b393b-f46f-45fa-8ba6-a74f9dfb01bd",
      "metadata": {
        "id": "df4b393b-f46f-45fa-8ba6-a74f9dfb01bd",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "# !pip install torch_fidelity"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "hza3oMk1xlJ8",
      "metadata": {
        "id": "hza3oMk1xlJ8"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import numpy as np\n",
        "import cv2\n",
        "import matplotlib.pyplot as plt\n",
        "import itertools\n",
        "import random\n",
        "from torchvision.utils import make_grid\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from PIL import Image\n",
        "import torchvision.transforms as transforms\n",
        "import torch.nn.functional as F\n",
        "import pandas as pd\n",
        "import torch_fidelity\n",
        "from tkinter import Variable\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8bf85d24",
      "metadata": {},
      "outputs": [],
      "source": [
        "SEED = 42\n",
        "print(\"Random Seed:\", SEED)\n",
        "random.seed(SEED)\n",
        "np.random.seed(SEED)\n",
        "torch_seed = torch.initial_seed()\n",
        "torch.manual_seed(torch_seed)\n",
        "print(\"Torch seed:\" , torch_seed)\n",
        "\n",
        "torch.backends.cudnn.deterministic = True\n",
        "torch.backends.cudnn.benchmark = False\n",
        "\n",
        "cuda = torch.cuda.is_available()\n",
        "Tensor = torch.cuda.FloatTensor if cuda else torch.Tensor"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "lZfcSKr9xnuf",
      "metadata": {
        "id": "lZfcSKr9xnuf"
      },
      "source": [
        "## 2. Define your model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d0710591",
      "metadata": {},
      "outputs": [],
      "source": [
        "class ChannelAttention(nn.Module):\n",
        "    def __init__(self, in_channels, reduction_ratio=16):\n",
        "        super(ChannelAttention, self).__init__()\n",
        "        self.avg_pool = nn.AdaptiveAvgPool2d(1)\n",
        "        self.max_pool = nn.AdaptiveMaxPool2d(1)\n",
        "        \n",
        "        self.fc = nn.Sequential(\n",
        "            nn.Conv2d(in_channels, in_channels // reduction_ratio, 1, bias=False),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Conv2d(in_channels // reduction_ratio, in_channels, 1, bias=False)\n",
        "        )\n",
        "        self.sigmoid = nn.Sigmoid()\n",
        "        \n",
        "    def forward(self, x):\n",
        "        avg_out = self.fc(self.avg_pool(x))\n",
        "        max_out = self.fc(self.max_pool(x))\n",
        "        out = avg_out + max_out\n",
        "        return self.sigmoid(out)\n",
        "\n",
        "class SpatialAttention(nn.Module):\n",
        "    def __init__(self, kernel_size=7):\n",
        "        super(SpatialAttention, self).__init__()\n",
        "        self.conv = nn.Conv2d(2, 1, kernel_size, padding=kernel_size//2, bias=False)\n",
        "        self.sigmoid = nn.Sigmoid()\n",
        "        \n",
        "    def forward(self, x):\n",
        "        avg_out = torch.mean(x, dim=1, keepdim=True)\n",
        "        max_out, _ = torch.max(x, dim=1, keepdim=True)\n",
        "        x_cat = torch.cat([avg_out, max_out], dim=1)\n",
        "        out = self.conv(x_cat)\n",
        "        return self.sigmoid(out)\n",
        "\n",
        "class CBAM(nn.Module):\n",
        "    def __init__(self, channels, reduction_ratio=16, kernel_size=7):\n",
        "        super(CBAM, self).__init__()\n",
        "        self.channel_att = ChannelAttention(channels, reduction_ratio)\n",
        "        self.spatial_att = SpatialAttention(kernel_size)\n",
        "        \n",
        "    def forward(self, x):\n",
        "        x = x * self.channel_att(x)\n",
        "        x = x * self.spatial_att(x)\n",
        "        return x\n",
        "\n",
        "class ImprovedResidualBlock(nn.Module):\n",
        "    def __init__(self, in_features, use_cbam=True):\n",
        "        super(ImprovedResidualBlock, self).__init__()\n",
        "        \n",
        "        self.block = nn.Sequential(\n",
        "            nn.ReflectionPad2d(1),\n",
        "            nn.Conv2d(in_features, in_features, kernel_size=3, padding=0),\n",
        "            nn.InstanceNorm2d(in_features),\n",
        "            nn.LeakyReLU(0.2, inplace=True),\n",
        "            nn.ReflectionPad2d(1),\n",
        "            nn.Conv2d(in_features, in_features, kernel_size=3, padding=0),\n",
        "            nn.InstanceNorm2d(in_features)\n",
        "        )\n",
        "        \n",
        "        # Add both SE and CBAM attention\n",
        "        self.se = SqueezeExcitation(in_features)\n",
        "        self.use_cbam = use_cbam\n",
        "        if use_cbam:\n",
        "            self.cbam = CBAM(in_features)\n",
        "    \n",
        "    def forward(self, x):\n",
        "        out = self.block(x)\n",
        "        out = self.se(out)  # Apply squeeze-excitation\n",
        "        if self.use_cbam:\n",
        "            out = self.cbam(out)  # Apply CBAM\n",
        "        return x + out  # Skip connection\n",
        "\n",
        "class SqueezeExcitation(nn.Module):\n",
        "    def __init__(self, channel, reduction=16):\n",
        "        super(SqueezeExcitation, self).__init__()\n",
        "        self.avg_pool = nn.AdaptiveAvgPool2d(1)\n",
        "        self.fc = nn.Sequential(\n",
        "            nn.Linear(channel, channel // reduction, bias=False),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Linear(channel // reduction, channel, bias=False),\n",
        "            nn.Sigmoid()\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        b, c, _, _ = x.size()\n",
        "        y = self.avg_pool(x).view(b, c)\n",
        "        y = self.fc(y).view(b, c, 1, 1)\n",
        "        return x * y.expand_as(x)\n",
        "\n",
        "class EnhancedSelfAttention(nn.Module):\n",
        "    def __init__(self, in_dim):\n",
        "        super(EnhancedSelfAttention, self).__init__()\n",
        "        # Standard self-attention components\n",
        "        self.query_conv = nn.Conv2d(in_dim, in_dim//8, kernel_size=1)\n",
        "        self.key_conv = nn.Conv2d(in_dim, in_dim//8, kernel_size=1)\n",
        "        self.value_conv = nn.Conv2d(in_dim, in_dim, kernel_size=1)\n",
        "        \n",
        "        # Positional bias for facial regions (center-weighted attention)\n",
        "        self.pos_bias = nn.Parameter(torch.zeros(1, 1, 64, 64))  # For 256x256 inputs after downsampling\n",
        "        nn.init.normal_(self.pos_bias, 0, 0.02)\n",
        "        \n",
        "        self.gamma = nn.Parameter(torch.zeros(1))\n",
        "        self.softmax = nn.Softmax(dim=-1)\n",
        "        \n",
        "    def forward(self, x):\n",
        "        batch_size, C, width, height = x.size()\n",
        "        \n",
        "        # Create position bias (emphasize center where face usually is)\n",
        "        if width != self.pos_bias.shape[2] or height != self.pos_bias.shape[3]:\n",
        "            pos_bias = F.interpolate(self.pos_bias, size=(width, height), mode='bilinear')\n",
        "        else:\n",
        "            pos_bias = self.pos_bias\n",
        "            \n",
        "        # Project to get query, key, value\n",
        "        query = self.query_conv(x).view(batch_size, -1, width*height).permute(0, 2, 1)\n",
        "        key = self.key_conv(x).view(batch_size, -1, width*height)\n",
        "        value = self.value_conv(x).view(batch_size, -1, width*height)\n",
        "        \n",
        "        # Calculate attention map with positional bias\n",
        "        attention = torch.bmm(query, key)\n",
        "        # Add positional bias to attention scores (reshape pos_bias to match attention dimensions)\n",
        "        attention = attention + pos_bias.view(1, width*height, width*height)\n",
        "        attention = self.softmax(attention)\n",
        "        \n",
        "        # Apply attention to value\n",
        "        out = torch.bmm(value, attention.permute(0, 2, 1))\n",
        "        out = out.view(batch_size, C, width, height)\n",
        "        \n",
        "        # Apply gamma parameter and add to input\n",
        "        out = self.gamma * out + x\n",
        "        return out\n",
        "    \n",
        "class AdaptiveInstanceNorm(nn.Module):\n",
        "    def __init__(self, in_channel):\n",
        "        super().__init__()\n",
        "        self.norm = nn.InstanceNorm2d(in_channel, affine=False)\n",
        "        \n",
        "    def forward(self, x, style_scale, style_bias):\n",
        "        out = self.norm(x)\n",
        "        # Apply learned scale and bias from style information\n",
        "        out = style_scale * out + style_bias\n",
        "        return out\n",
        "\n",
        "class StyleModulatedResBlock(nn.Module):\n",
        "    def __init__(self, in_features):\n",
        "        super(StyleModulatedResBlock, self).__init__()\n",
        "        \n",
        "        # Style modulation parameters\n",
        "        self.style_scale = nn.Parameter(torch.ones(1, in_features, 1, 1))\n",
        "        self.style_bias = nn.Parameter(torch.zeros(1, in_features, 1, 1))\n",
        "        \n",
        "        # Main conv blocks\n",
        "        self.conv1 = nn.Sequential(\n",
        "            nn.ReflectionPad2d(1),\n",
        "            nn.Conv2d(in_features, in_features, kernel_size=3, padding=0),\n",
        "        )\n",
        "        self.adain1 = AdaptiveInstanceNorm(in_features)\n",
        "        self.act1 = nn.LeakyReLU(0.2, inplace=True)\n",
        "        \n",
        "        self.conv2 = nn.Sequential(\n",
        "            nn.ReflectionPad2d(1),\n",
        "            nn.Conv2d(in_features, in_features, kernel_size=3, padding=0),\n",
        "        )\n",
        "        self.adain2 = AdaptiveInstanceNorm(in_features)\n",
        "        \n",
        "        # Add SE attention\n",
        "        self.se = SqueezeExcitation(in_features)\n",
        "        \n",
        "    def forward(self, x):\n",
        "        residual = x\n",
        "        \n",
        "        out = self.conv1(x)\n",
        "        out = self.adain1(out, self.style_scale, self.style_bias)\n",
        "        out = self.act1(out)\n",
        "        \n",
        "        out = self.conv2(out)\n",
        "        out = self.adain2(out, self.style_scale, self.style_bias)\n",
        "        \n",
        "        # Apply SE attention\n",
        "        out = self.se(out)\n",
        "        \n",
        "        return residual + out"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7e14789e",
      "metadata": {},
      "outputs": [],
      "source": [
        "class Discriminator(nn.Module):\n",
        "    def __init__(self, in_channels=3, features_d=64):\n",
        "        super(Discriminator, self).__init__()\n",
        "        \n",
        "        self.scale_factor = 16\n",
        "        \n",
        "        # Feature extraction layers - shared between patch and global discriminators\n",
        "        self.feature_extraction = nn.Sequential(\n",
        "            nn.utils.spectral_norm(nn.Conv2d(in_channels, features_d, kernel_size=4, stride=2, padding=1)),\n",
        "            nn.LeakyReLU(0.2, inplace=True),\n",
        "            \n",
        "            nn.utils.spectral_norm(nn.Conv2d(features_d, features_d*2, kernel_size=4, stride=2, padding=1)),\n",
        "            nn.InstanceNorm2d(features_d*2),\n",
        "            nn.LeakyReLU(0.2, inplace=True),\n",
        "            \n",
        "            nn.utils.spectral_norm(nn.Conv2d(features_d*2, features_d*4, kernel_size=4, stride=2, padding=1)),\n",
        "            nn.InstanceNorm2d(features_d*4),\n",
        "            nn.LeakyReLU(0.2, inplace=True),\n",
        "            \n",
        "            nn.utils.spectral_norm(nn.Conv2d(features_d*4, features_d*8, kernel_size=4, stride=2, padding=1)),\n",
        "            nn.InstanceNorm2d(features_d*8),\n",
        "            nn.LeakyReLU(0.2, inplace=True)\n",
        "        )\n",
        "        \n",
        "        # PatchGAN output\n",
        "        self.patch_output = nn.Sequential(\n",
        "            nn.utils.spectral_norm(nn.Conv2d(features_d*8, 1, kernel_size=4, stride=1, padding=1))\n",
        "        )\n",
        "        \n",
        "        # Global discriminator output\n",
        "        self.global_output = nn.Sequential(\n",
        "            nn.AdaptiveAvgPool2d(1),\n",
        "            nn.Flatten(),\n",
        "            nn.Linear(features_d*8, 1)\n",
        "        )\n",
        "    \n",
        "    def forward(self, x, get_features=False):\n",
        "        features = self.feature_extraction(x)\n",
        "        patch_out = self.patch_output(features)\n",
        "        global_out = self.global_output(features)\n",
        "        \n",
        "        if get_features:\n",
        "            # Return intermediate features for feature matching loss\n",
        "            return patch_out, global_out, [features]\n",
        "        return patch_out, global_out"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "adeb173f",
      "metadata": {},
      "outputs": [],
      "source": [
        "class Generator(nn.Module):\n",
        "    def __init__(self, in_channels=3, num_residual_blocks=9):\n",
        "        super(Generator, self).__init__()\n",
        "        \n",
        "        # Initial convolution block\n",
        "        model = [\n",
        "            nn.ReflectionPad2d(3),\n",
        "            nn.Conv2d(in_channels, 64, kernel_size=7, padding=0),\n",
        "            nn.InstanceNorm2d(64),\n",
        "            nn.ReLU(inplace=True)\n",
        "        ]\n",
        "        \n",
        "        # Enhanced downsampling\n",
        "        in_features = 64\n",
        "        out_features = in_features * 2\n",
        "        for _ in range(2):\n",
        "            model += [\n",
        "                nn.Conv2d(in_features, out_features, kernel_size=3, stride=2, padding=1),\n",
        "                nn.InstanceNorm2d(out_features),\n",
        "                nn.ReLU(inplace=True)\n",
        "            ]\n",
        "            in_features = out_features\n",
        "            out_features = in_features * 2\n",
        "        \n",
        "        # Use a mix of improved blocks\n",
        "        for i in range(num_residual_blocks):\n",
        "            if i % 3 == 0:  # Every 3rd block uses style modulation\n",
        "                model += [StyleModulatedResBlock(in_features)]\n",
        "            else:\n",
        "                use_cbam = (i >= 3 and i <= 6)\n",
        "                model += [ImprovedResidualBlock(in_features, use_cbam=use_cbam)]\n",
        "        \n",
        "        # Add enhanced self-attention after residual blocks\n",
        "        model += [EnhancedSelfAttention(in_features)]\n",
        "        \n",
        "        # Enhanced upsampling pathway\n",
        "        out_features = in_features // 2\n",
        "        for _ in range(2):\n",
        "            model += [\n",
        "                nn.ConvTranspose2d(in_features, out_features, kernel_size=3, stride=2, padding=1, output_padding=1),\n",
        "                nn.InstanceNorm2d(out_features),\n",
        "                nn.ReLU(inplace=True)\n",
        "            ]\n",
        "            in_features = out_features\n",
        "            out_features = in_features // 2\n",
        "        \n",
        "        # Output layer\n",
        "        model += [\n",
        "            nn.ReflectionPad2d(3),\n",
        "            nn.Conv2d(64, in_channels, kernel_size=7, padding=0),\n",
        "            nn.Tanh()\n",
        "        ]\n",
        "        \n",
        "        self.model = nn.Sequential(*model)\n",
        "    \n",
        "    def forward(self, x):\n",
        "        return self.model(x)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "efd434ad",
      "metadata": {},
      "outputs": [],
      "source": [
        "def relativistic_loss(real_pred, fake_pred, criterion):\n",
        "    # Standard GAN uses D(real) and D(fake) independently\n",
        "    # Relativistic GAN compares real to fake samples\n",
        "    \n",
        "    # For patch-level predictions\n",
        "    real_patch, real_global = real_pred\n",
        "    fake_patch, fake_global = fake_pred\n",
        "    \n",
        "    # Relativistic average for patch outputs\n",
        "    real_fake_diff_patch = real_patch - fake_patch.mean(0, keepdim=True)\n",
        "    fake_real_diff_patch = fake_patch - real_patch.mean(0, keepdim=True)\n",
        "    \n",
        "    # Relativistic average for global outputs\n",
        "    real_fake_diff_global = real_global - fake_global.mean(0, keepdim=True)\n",
        "    fake_real_diff_global = fake_global - real_global.mean(0, keepdim=True)\n",
        "    \n",
        "    # Combine losses\n",
        "    batch_size = real_global.size(0)\n",
        "    ones = torch.ones(batch_size, 1).cuda() if torch.cuda.is_available() else torch.ones(batch_size, 1)\n",
        "    zeros = torch.zeros(batch_size, 1).cuda() if torch.cuda.is_available() else torch.zeros(batch_size, 1)\n",
        "    \n",
        "    # Patch level loss\n",
        "    loss_real_patch = criterion(real_fake_diff_patch, torch.ones_like(real_fake_diff_patch))\n",
        "    loss_fake_patch = criterion(fake_real_diff_patch, torch.zeros_like(fake_real_diff_patch))\n",
        "    \n",
        "    # Global level loss\n",
        "    loss_real_global = nn.BCEWithLogitsLoss()(real_fake_diff_global, ones)\n",
        "    loss_fake_global = nn.BCEWithLogitsLoss()(fake_real_diff_global, zeros)\n",
        "    \n",
        "    # Combine patch and global losses\n",
        "    loss_real = 0.7 * loss_real_patch + 0.3 * loss_real_global\n",
        "    loss_fake = 0.7 * loss_fake_patch + 0.3 * loss_fake_global\n",
        "    \n",
        "    return (loss_real + loss_fake) / 2\n",
        "\n",
        "def compute_gradient_penalty(discriminator, real_samples, fake_samples):\n",
        "    # Random interpolation of real and fake samples\n",
        "    alpha = torch.rand((real_samples.size(0), 1, 1, 1)).type(Tensor)\n",
        "    interpolates = (alpha * real_samples + (1 - alpha) * fake_samples).requires_grad_(True)\n",
        "    \n",
        "    # Get discriminator output for interpolated images\n",
        "    patch_interpolates, global_interpolates = discriminator(interpolates)\n",
        "    \n",
        "    # Set fake gradients for patch outputs\n",
        "    fake_patch_grad = torch.ones_like(patch_interpolates).type(Tensor)\n",
        "    patch_gradients = torch.autograd.grad(\n",
        "        outputs=patch_interpolates,\n",
        "        inputs=interpolates,\n",
        "        grad_outputs=fake_patch_grad,\n",
        "        create_graph=True,\n",
        "        retain_graph=True,\n",
        "        only_inputs=True,\n",
        "    )[0]\n",
        "    \n",
        "    # Flatten gradients and compute penalty\n",
        "    patch_gradients = patch_gradients.reshape(real_samples.size(0), -1)\n",
        "    patch_gradient_penalty = ((patch_gradients.norm(2, dim=1) - 1) ** 2).mean()\n",
        "    \n",
        "    # Set fake gradients for global outputs\n",
        "    fake_global_grad = torch.ones_like(global_interpolates).type(Tensor)\n",
        "    global_gradients = torch.autograd.grad(\n",
        "        outputs=global_interpolates,\n",
        "        inputs=interpolates,\n",
        "        grad_outputs=fake_global_grad,\n",
        "        create_graph=True,\n",
        "        retain_graph=True,\n",
        "        only_inputs=True,\n",
        "    )[0]\n",
        "    \n",
        "    # Flatten gradients and compute penalty\n",
        "    global_gradients = global_gradients.reshape(real_samples.size(0), -1)\n",
        "    global_gradient_penalty = ((global_gradients.norm(2, dim=1) - 1) ** 2).mean()\n",
        "    \n",
        "    # Combined penalty\n",
        "    return 0.7 * patch_gradient_penalty + 0.3 * global_gradient_penalty\n",
        "\n",
        "def feature_matching_loss(real_features, fake_features):\n",
        "    \"\"\"\n",
        "    Calculate feature matching loss by comparing features extracted from real and fake images.\n",
        "    \n",
        "    Args:\n",
        "        real_features: List of feature tensors from real images\n",
        "        fake_features: List of feature tensors from fake images\n",
        "        \n",
        "    Returns:\n",
        "        The mean absolute difference between real and fake features\n",
        "    \"\"\"\n",
        "    # Initialize loss\n",
        "    loss = 0.0\n",
        "    \n",
        "    # Both should be lists of the same length, containing feature maps\n",
        "    assert len(real_features) == len(fake_features), \"Feature lists must have same length\"\n",
        "    \n",
        "    # Calculate L1 loss between corresponding feature maps\n",
        "    for real_feat, fake_feat in zip(real_features, fake_features):\n",
        "        # Make sure feature maps have same shape\n",
        "        assert real_feat.shape == fake_feat.shape, f\"Feature shapes don't match: {real_feat.shape} vs {fake_feat.shape}\"\n",
        "        # Calculate mean absolute error between feature maps\n",
        "        loss += torch.mean(torch.abs(real_feat - fake_feat))\n",
        "    \n",
        "    # Average over number of feature maps\n",
        "    return loss / len(real_features)\n",
        "\n",
        "def fft_loss(real_image, fake_image, alpha=1.0):\n",
        "    \"\"\"\n",
        "    Calculate loss in frequency domain to better preserve structure and style\n",
        "    \"\"\"\n",
        "    # Get FFT of images\n",
        "    real_fft = torch.fft.fft2(real_image, dim=(-2, -1))\n",
        "    fake_fft = torch.fft.fft2(fake_image, dim=(-2, -1))\n",
        "    \n",
        "    # Calculate magnitude and phase\n",
        "    real_magnitude = torch.abs(real_fft)\n",
        "    fake_magnitude = torch.abs(fake_fft)\n",
        "    \n",
        "    # Focus on low frequencies (structure) by creating a mask\n",
        "    batch, channels, height, width = real_image.shape\n",
        "    mask = torch.ones((batch, channels, height, width), device=real_image.device)\n",
        "    \n",
        "    # Create circular mask emphasizing lower frequencies\n",
        "    y, x = torch.meshgrid(torch.arange(height), torch.arange(width))\n",
        "    center_y, center_x = height // 2, width // 2\n",
        "    # Create distance matrix from center\n",
        "    dist = ((y - center_y) ** 2 + (x - center_x) ** 2).sqrt()\n",
        "    # Normalize distances to [0, 1]\n",
        "    dist = dist / dist.max()\n",
        "    # Create mask that emphasizes low frequencies (center of FFT)\n",
        "    freq_mask = (1 - dist).unsqueeze(0).unsqueeze(0).to(real_image.device)\n",
        "    \n",
        "    # Apply frequency mask (emphasize low-frequency differences)\n",
        "    masked_diff = (real_magnitude - fake_magnitude) * freq_mask\n",
        "    \n",
        "    # L1 loss on magnitude\n",
        "    loss = alpha * torch.mean(torch.abs(masked_diff))\n",
        "    return loss\n",
        "\n",
        "def edge_loss(real_images, fake_images, weight=1.0):\n",
        "    \"\"\"\n",
        "    Edge preservation loss using Sobel filters\n",
        "    \"\"\"\n",
        "    def sobel_filters(x):\n",
        "        sobel_x = torch.tensor([[-1, 0, 1], [-2, 0, 2], [-1, 0, 1]], \n",
        "                              dtype=torch.float32, device=x.device).reshape(1, 1, 3, 3).repeat(3, 1, 1, 1)\n",
        "        sobel_y = torch.tensor([[-1, -2, -1], [0, 0, 0], [1, 2, 1]], \n",
        "                              dtype=torch.float32, device=x.device).reshape(1, 1, 3, 3).repeat(3, 1, 1, 1)\n",
        "        \n",
        "        grad_x = F.conv2d(x, sobel_x, padding=1, groups=3)\n",
        "        grad_y = F.conv2d(x, sobel_y, padding=1, groups=3)\n",
        "        \n",
        "        return torch.sqrt(grad_x**2 + grad_y**2 + 1e-8)  # Add small epsilon for numerical stability\n",
        "    \n",
        "    # Get edges\n",
        "    real_edges = sobel_filters(real_images)\n",
        "    fake_edges = sobel_filters(fake_images)\n",
        "    \n",
        "    # L1 loss on edges\n",
        "    return weight * F.l1_loss(real_edges, fake_edges)\n",
        "\n",
        "def color_histogram_loss(real_images, fake_images, nbins=64):\n",
        "    \"\"\"\n",
        "    Ensure consistent color distribution between domains\n",
        "    \"\"\"\n",
        "    def histogram(x, nbins):\n",
        "        batch_size, channels, height, width = x.shape\n",
        "        \n",
        "        # Scale to [0, 1]\n",
        "        x = (x + 1) / 2\n",
        "        \n",
        "        # Compute histogram for each channel\n",
        "        hist_list = []\n",
        "        for c in range(channels):\n",
        "            channel_data = x[:, c].reshape(batch_size, -1)  # Flatten spatial dimensions\n",
        "            # Create histogram bins\n",
        "            hist = torch.zeros(batch_size, nbins, device=x.device)\n",
        "            \n",
        "            for b in range(batch_size):\n",
        "                # Compute histogram using binning\n",
        "                for i in range(nbins):\n",
        "                    bin_start = i / nbins\n",
        "                    bin_end = (i + 1) / nbins\n",
        "                    # Count pixels in this bin\n",
        "                    mask = (channel_data[b] >= bin_start) & (channel_data[b] < bin_end)\n",
        "                    hist[b, i] = mask.float().sum() / (height * width)\n",
        "            hist_list.append(hist)\n",
        "        \n",
        "        # Concatenate histograms from all channels\n",
        "        return torch.cat(hist_list, dim=1)\n",
        "    \n",
        "    # Compute color histograms\n",
        "    real_hist = histogram(real_images, nbins)\n",
        "    fake_hist = histogram(fake_images, nbins)\n",
        "    \n",
        "    # Earth Mover's Distance approximation\n",
        "    diff = torch.abs(torch.cumsum(real_hist, dim=1) - torch.cumsum(fake_hist, dim=1))\n",
        "    return torch.mean(diff)\n",
        "\n",
        "# Create LR scheduler for generators and discriminators\n",
        "def get_lr_schedulers(optimizer_G, optimizer_D_A, optimizer_D_B, num_epochs):\n",
        "    # Linear decay learning rate scheduler\n",
        "    def lambda_rule(epoch):\n",
        "        # Linearly decrease learning rate to 0 over num_epochs\n",
        "        return 1.0 - max(0, epoch - num_epochs // 2) / float(num_epochs // 2)\n",
        "    \n",
        "    scheduler_G = torch.optim.lr_scheduler.LambdaLR(optimizer_G, lr_lambda=lambda_rule)\n",
        "    scheduler_D_A = torch.optim.lr_scheduler.LambdaLR(optimizer_D_A, lr_lambda=lambda_rule)\n",
        "    scheduler_D_B = torch.optim.lr_scheduler.LambdaLR(optimizer_D_B, lr_lambda=lambda_rule)\n",
        "    \n",
        "    return scheduler_G, scheduler_D_A, scheduler_D_B\n",
        "\n",
        "class AdaptiveLossWeights:\n",
        "    def __init__(self, initial_weights):\n",
        "        \"\"\"\n",
        "        Adaptively adjust loss weights during training\n",
        "        \n",
        "        Args:\n",
        "            initial_weights: dict of initial weights for different loss components\n",
        "        \"\"\"\n",
        "        self.weights = initial_weights\n",
        "        self.loss_history = {k: [] for k in initial_weights}\n",
        "        self.window_size = 50  # Moving average window\n",
        "        \n",
        "    def update(self, current_losses):\n",
        "        \"\"\"Update weights based on recent loss trends\"\"\"\n",
        "        # Add current losses to history\n",
        "        for loss_name, loss_value in current_losses.items():\n",
        "            if loss_name in self.loss_history:\n",
        "                self.loss_history[loss_name].append(loss_value)\n",
        "                # Keep only recent history\n",
        "                if len(self.loss_history[loss_name]) > self.window_size:\n",
        "                    self.loss_history[loss_name].pop(0)\n",
        "        \n",
        "        # Only update if we have enough history\n",
        "        if all(len(hist) >= self.window_size for hist in self.loss_history.values()):\n",
        "            # Get average of recent losses\n",
        "            avg_losses = {k: sum(v[-self.window_size:]) / self.window_size \n",
        "                         for k, v in self.loss_history.items()}\n",
        "            \n",
        "            # Calculate relative magnitudes\n",
        "            total_loss = sum(avg_losses.values())\n",
        "            loss_ratios = {k: v / total_loss for k, v in avg_losses.items()}\n",
        "            \n",
        "            # Adjust weights inversely to loss ratios (larger loss -> smaller weight)\n",
        "            target_ratio = 1.0 / len(self.weights)\n",
        "            for loss_name in self.weights:\n",
        "                if loss_ratios[loss_name] > target_ratio * 1.5:  # Loss is too dominant\n",
        "                    self.weights[loss_name] *= 0.9  # Decrease weight\n",
        "                elif loss_ratios[loss_name] < target_ratio * 0.5:  # Loss is too small\n",
        "                    self.weights[loss_name] *= 1.1  # Increase weight\n",
        "                    \n",
        "        return self.weights"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c9cb9af6",
      "metadata": {},
      "outputs": [],
      "source": [
        "class ImagePool:\n",
        "    def __init__(self, pool_size=50):\n",
        "        \"\"\"Initialize the ImagePool with a specified pool size.\"\"\"\n",
        "        self.pool_size = pool_size\n",
        "        self.num_imgs = 0\n",
        "        self.images = []\n",
        "\n",
        "    def query(self, images):\n",
        "        \"\"\"\n",
        "        Returns images from the pool.\n",
        "        If the pool is not full, add the incoming images.\n",
        "        Otherwise, randomly choose to return an old image or the current one.\n",
        "        \"\"\"\n",
        "        if self.pool_size == 0:\n",
        "            return images\n",
        "        return_images = []\n",
        "        for image in images:\n",
        "            image = torch.unsqueeze(image.data, 0)  # add a batch dimension if needed\n",
        "            if self.num_imgs < self.pool_size:\n",
        "                self.images.append(image)\n",
        "                self.num_imgs += 1\n",
        "                return_images.append(image)\n",
        "            else:\n",
        "                # With probability 0.5, use a previously stored image\n",
        "                if random.uniform(0, 1) > 0.5:\n",
        "                    idx = random.randint(0, self.pool_size - 1)\n",
        "                    tmp = self.images[idx].clone()\n",
        "                    self.images[idx] = image\n",
        "                    return_images.append(tmp)\n",
        "                else:\n",
        "                    return_images.append(image)\n",
        "        return torch.cat(return_images, 0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "db10b05b",
      "metadata": {},
      "outputs": [],
      "source": [
        "def print_epoch_losses(epoch, gen_losses, gen_AB_losses, gen_BA_losses, id_losses, gan_losses, cycle_losses, fm_losses, discA_losses, discB_losses):\n",
        "    # Calculate average losses for the epoch\n",
        "    avg_gen_loss = sum(gen_losses) / len(gen_losses)\n",
        "    avg_gen_AB_loss = sum(gen_AB_losses) / len(gen_AB_losses)\n",
        "    avg_gen_BA_loss = sum(gen_BA_losses) / len(gen_BA_losses)\n",
        "    avg_id_loss = sum(id_losses) / len(id_losses)\n",
        "    avg_gan_loss = sum(gan_losses) / len(gan_losses)\n",
        "    avg_cycle_loss = sum(cycle_losses) / len(cycle_losses)\n",
        "    avg_fm_loss = sum(fm_losses) / len(fm_losses)\n",
        "    avg_discA_loss = sum(discA_losses) / len(discA_losses)\n",
        "    avg_discB_loss = sum(discB_losses) / len(discB_losses)\n",
        "    \n",
        "    print(f\"Epoch {epoch}:\")\n",
        "    print(f\"  Generator Total Loss: {avg_gen_loss:.4f}\")\n",
        "    print(f\"    G_AB (A->B) Loss:  {avg_gen_AB_loss:.4f}\")\n",
        "    print(f\"    G_BA (B->A) Loss:  {avg_gen_BA_loss:.4f}\")\n",
        "    print(f\"    Identity Loss:     {avg_id_loss:.4f}\")\n",
        "    print(f\"    GAN Loss:          {avg_gan_loss:.4f}\")\n",
        "    print(f\"    Cycle Loss:        {avg_cycle_loss:.4f}\")\n",
        "    print(f\"    Feature Match:     {avg_fm_loss:.4f}\")\n",
        "    print(f\"  Discriminator A Loss: {avg_discA_loss:.4f}\")\n",
        "    print(f\"  Discriminator B Loss: {avg_discB_loss:.4f}\")\n",
        "\n",
        "def plot_losses(gen_loss_history, gen_AB_loss_history, gen_BA_loss_history, \n",
        "                id_loss_history, gan_loss_history, cycle_loss_history, fm_loss_history,\n",
        "                discA_loss_history, discB_loss_history):\n",
        "    epochs = range(1, len(gen_loss_history) + 1)\n",
        "    \n",
        "    plt.figure(figsize=(15, 10))\n",
        "    \n",
        "    # Plot Generator Losses\n",
        "    plt.subplot(2, 2, 1)\n",
        "    plt.plot(epochs, gen_loss_history, label=\"Total Generator Loss\", color=\"blue\")\n",
        "    plt.plot(epochs, gen_AB_loss_history, label=\"G_AB Loss\", color=\"cyan\", linestyle=\"dashed\")\n",
        "    plt.plot(epochs, gen_BA_loss_history, label=\"G_BA Loss\", color=\"magenta\", linestyle=\"dashed\")\n",
        "    plt.xlabel(\"Epochs\")\n",
        "    plt.ylabel(\"Loss\")\n",
        "    plt.title(\"Generator Total Losses\")\n",
        "    plt.legend()\n",
        "    plt.grid()\n",
        "    \n",
        "    # Plot Component Losses\n",
        "    plt.subplot(2, 2, 2)\n",
        "    plt.plot(epochs, id_loss_history, label=\"Identity Loss\", color=\"green\")\n",
        "    plt.plot(epochs, gan_loss_history, label=\"GAN Loss\", color=\"red\")\n",
        "    plt.plot(epochs, cycle_loss_history, label=\"Cycle Loss\", color=\"orange\")\n",
        "    plt.plot(epochs, fm_loss_history, label=\"Feature Match Loss\", color=\"purple\")\n",
        "    plt.xlabel(\"Epochs\")\n",
        "    plt.ylabel(\"Loss\")\n",
        "    plt.title(\"Loss Components\")\n",
        "    plt.legend()\n",
        "    plt.grid()\n",
        "    \n",
        "    # Plot Discriminator Losses\n",
        "    plt.subplot(2, 2, 3)\n",
        "    plt.plot(epochs, discA_loss_history, label=\"Discriminator A Loss\", color=\"teal\")\n",
        "    plt.plot(epochs, discB_loss_history, label=\"Discriminator B Loss\", color=\"brown\")\n",
        "    plt.xlabel(\"Epochs\")\n",
        "    plt.ylabel(\"Loss\")\n",
        "    plt.title(\"Discriminator Losses\")\n",
        "    plt.legend()\n",
        "    plt.grid()\n",
        "    \n",
        "    # Plot combined important losses for overall view\n",
        "    plt.subplot(2, 2, 4)\n",
        "    plt.plot(epochs, gen_loss_history, label=\"Generator Loss\", color=\"blue\")\n",
        "    plt.plot(epochs, cycle_loss_history, label=\"Cycle Loss\", color=\"orange\")\n",
        "    plt.plot(epochs, gan_loss_history, label=\"GAN Loss\", color=\"red\")\n",
        "    plt.plot(epochs, [(a+b)/2 for a, b in zip(discA_loss_history, discB_loss_history)], \n",
        "             label=\"Avg Discriminator Loss\", color=\"purple\")\n",
        "    plt.xlabel(\"Epochs\")\n",
        "    plt.ylabel(\"Loss\")\n",
        "    plt.title(\"Key Losses\")\n",
        "    plt.legend()\n",
        "    plt.grid()\n",
        "    \n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "def show_images(real_A, fake_B, recov_A, real_B, fake_A, recov_B, size=(256, 256), num_samples=5):\n",
        "    \"\"\"Display three rows of images: real A, fake B, reconstructed A, real B, fake A, reconstructed B\"\"\"\n",
        "    # Denormalize images from [-1, 1] to [0, 1]\n",
        "    def denorm(tensor):\n",
        "        return (tensor * 0.5 + 0.5).clamp(0, 1)\n",
        "    \n",
        "    # Create a figure with 3 rows of images\n",
        "    fig, axes = plt.subplots(2, 3, figsize=(15, 10))\n",
        "    \n",
        "    # Only use a subset of images\n",
        "    real_A = denorm(real_A[:num_samples])\n",
        "    fake_B = denorm(fake_B[:num_samples])\n",
        "    recov_A = denorm(recov_A[:num_samples])\n",
        "    real_B = denorm(real_B[:num_samples])\n",
        "    fake_A = denorm(fake_A[:num_samples])\n",
        "    recov_B = denorm(recov_B[:num_samples])\n",
        "    \n",
        "    # Create grids from batches of images\n",
        "    real_A_grid = make_grid(real_A, nrow=num_samples).permute(1, 2, 0).cpu().numpy()\n",
        "    fake_B_grid = make_grid(fake_B, nrow=num_samples).permute(1, 2, 0).cpu().numpy()\n",
        "    recov_A_grid = make_grid(recov_A, nrow=num_samples).permute(1, 2, 0).cpu().numpy()\n",
        "    real_B_grid = make_grid(real_B, nrow=num_samples).permute(1, 2, 0).cpu().numpy()\n",
        "    fake_A_grid = make_grid(fake_A, nrow=num_samples).permute(1, 2, 0).cpu().numpy()\n",
        "    recov_B_grid = make_grid(recov_B, nrow=num_samples).permute(1, 2, 0).cpu().numpy()\n",
        "    \n",
        "    # Display the images\n",
        "    axes[0, 0].imshow(real_A_grid)\n",
        "    axes[0, 0].set_title(\"Real A (Domain A)\")\n",
        "    axes[0, 0].axis(\"off\")\n",
        "    \n",
        "    axes[0, 1].imshow(fake_B_grid)\n",
        "    axes[0, 1].set_title(\"Fake B (Generated from A)\")\n",
        "    axes[0, 1].axis(\"off\")\n",
        "    \n",
        "    axes[0, 2].imshow(recov_A_grid)\n",
        "    axes[0, 2].set_title(\"Reconstructed A\")\n",
        "    axes[0, 2].axis(\"off\")\n",
        "    \n",
        "    axes[1, 0].imshow(real_B_grid)\n",
        "    axes[1, 0].set_title(\"Real B (Domain B)\")\n",
        "    axes[1, 0].axis(\"off\")\n",
        "    \n",
        "    axes[1, 1].imshow(fake_A_grid)\n",
        "    axes[1, 1].set_title(\"Fake A (Generated from B)\")\n",
        "    axes[1, 1].axis(\"off\")\n",
        "    \n",
        "    axes[1, 2].imshow(recov_B_grid)\n",
        "    axes[1, 2].set_title(\"Reconstructed B\")\n",
        "    axes[1, 2].axis(\"off\")\n",
        "    \n",
        "    plt.tight_layout()\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8dd7ff02",
      "metadata": {},
      "outputs": [],
      "source": [
        "image_size = (256, 256)\n",
        "batch_size = 4\n",
        "n_epochs = 100\n",
        "lr = 2e-4\n",
        "betas = (0.5, 0.999)\n",
        "lambda_cyc = 10.0\n",
        "lambda_identity = 5.0\n",
        "lambda_gan = 1.0\n",
        "lambda_fm = 5.0\n",
        "lambda_gp = 10.0\n",
        "pool_size_A = 50\n",
        "pool_size_B = 50\n",
        "num_residual_blocks = 9\n",
        "use_attention = True\n",
        "features_d = 64\n",
        "file_name = \"EnhancedCycleGAN-256x256\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "27c68638",
      "metadata": {},
      "outputs": [],
      "source": [
        "def get_face_specific_transforms(img_size=256):\n",
        "    \"\"\"\n",
        "    Create transforms specifically for face-to-cartoon translation\n",
        "    \"\"\"\n",
        "    return transforms.Compose([\n",
        "        # Fundamental transformations\n",
        "        transforms.Resize((img_size + 30, img_size + 30)),  # Resize larger for crop\n",
        "        transforms.RandomCrop((img_size, img_size)),  # Random crop for position variety\n",
        "        transforms.RandomHorizontalFlip(),  # Flip faces (common augmentation)\n",
        "        \n",
        "        # Face-specific augmentations (mild to preserve facial structure)\n",
        "        transforms.ColorJitter(brightness=0.1, contrast=0.1, saturation=0.1, hue=0.05),\n",
        "        transforms.RandomAffine(\n",
        "            degrees=5,              # Mild rotation (faces are orientation-sensitive)\n",
        "            translate=(0.05, 0.05), # Slight translation\n",
        "            scale=(0.95, 1.05),     # Mild scaling\n",
        "            fill=0                  # Fill with black\n",
        "        ),\n",
        "        \n",
        "        # Prepare for model\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
        "    ])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "409ea73f-b928-4e03-bc93-65204998136c",
      "metadata": {
        "id": "409ea73f-b928-4e03-bc93-65204998136c",
        "trusted": true
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Total parameters in CycleGAN model: 27.30 million\n",
            "cuda: False\n"
          ]
        }
      ],
      "source": [
        "\n",
        "\"\"\"\n",
        "Step 3. Define Loss\n",
        "\"\"\"\n",
        "criterion_GAN = nn.MSELoss()  # For adversarial loss\n",
        "criterion_cycle = nn.L1Loss()  # For cycle consistency loss\n",
        "criterion_identity = nn.L1Loss()  # For identity loss\n",
        "\n",
        "\"\"\"\n",
        "Step 4. Initalize G and D¶\n",
        "\"\"\"\n",
        "G_AB = Generator(in_channels=3, num_residual_blocks=num_residual_blocks)\n",
        "G_BA = Generator(in_channels=3, num_residual_blocks=num_residual_blocks)\n",
        "D_A = Discriminator(in_channels=3, features_d=features_d)\n",
        "D_B = Discriminator(in_channels=3, features_d=features_d)\n",
        "\n",
        "## Total parameters in CycleGAN should be less than 60MB\n",
        "total_params = sum(p.numel() for p in G_AB.parameters()) + \\\n",
        "               sum(p.numel() for p in G_BA.parameters()) + \\\n",
        "               sum(p.numel() for p in D_A.parameters()) + \\\n",
        "               sum(p.numel() for p in D_B.parameters())\n",
        "\n",
        "\n",
        "\"\"\"\n",
        "# modification of parameters computation is forbidden\n",
        "\"\"\"\n",
        "total_params_million = total_params / (1024 * 1024)\n",
        "print(f'Total parameters in CycleGAN model: {total_params_million:.2f} million')\n",
        "\n",
        "cuda = torch.cuda.is_available()\n",
        "print(f'cuda: {cuda}')\n",
        "if cuda:\n",
        "    G_AB = G_AB.cuda()\n",
        "    D_B = D_B.cuda()\n",
        "    G_BA = G_BA.cuda()\n",
        "    D_A = D_A.cuda()\n",
        "\n",
        "criterion_GAN = criterion_GAN.cuda()\n",
        "criterion_cycle = criterion_cycle.cuda()\n",
        "criterion_identity = criterion_identity.cuda()\n",
        "\n",
        "\"\"\"\n",
        "Step 5. Configure Optimizers\n",
        "\"\"\"\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8407ab93",
      "metadata": {},
      "outputs": [],
      "source": [
        "# -------------------------\n",
        "# Save Model\n",
        "# -------------------------\n",
        "def save_checkpoint(model, model_name, torch_seed, checkpoint_dir='models'):\n",
        "    # Ensure the directory exists\n",
        "    os.makedirs(checkpoint_dir, exist_ok=True)\n",
        "\n",
        "    checkpoint = {\n",
        "        'model': model.state_dict(),\n",
        "        'torch_seed': torch_seed\n",
        "    }\n",
        "\n",
        "    filename = os.path.join(checkpoint_dir, f\"{file_name}-{model_name}-best.pth\")\n",
        "\n",
        "    # Save the checkpoint\n",
        "    torch.save(checkpoint, filename)\n",
        "    print(f\"Model saved to {filename}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "uradmrYEyJsV",
      "metadata": {
        "id": "uradmrYEyJsV"
      },
      "source": [
        "## 3. Load the data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "czvYlCLDyCAm",
      "metadata": {
        "id": "czvYlCLDyCAm"
      },
      "outputs": [
        {
          "ename": "FileNotFoundError",
          "evalue": "[WinError 3] The system cannot find the path specified: '/kaggle/input/group-project/image_image_translation\\\\VAE_generation/train'",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "Cell \u001b[1;32mIn[10], line 46\u001b[0m\n\u001b[0;32m     37\u001b[0m transforms_ \u001b[38;5;241m=\u001b[39m transforms\u001b[38;5;241m.\u001b[39mCompose([\n\u001b[0;32m     38\u001b[0m     transforms\u001b[38;5;241m.\u001b[39mResize(image_size),\n\u001b[0;32m     39\u001b[0m     transforms\u001b[38;5;241m.\u001b[39mToTensor(),\n\u001b[0;32m     40\u001b[0m     transforms\u001b[38;5;241m.\u001b[39mNormalize((\u001b[38;5;241m0.5\u001b[39m, \u001b[38;5;241m0.5\u001b[39m, \u001b[38;5;241m0.5\u001b[39m), (\u001b[38;5;241m0.5\u001b[39m, \u001b[38;5;241m0.5\u001b[39m, \u001b[38;5;241m0.5\u001b[39m))\n\u001b[0;32m     41\u001b[0m ])\n\u001b[0;32m     43\u001b[0m batch_size \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m5\u001b[39m\n\u001b[0;32m     45\u001b[0m trainloader \u001b[38;5;241m=\u001b[39m DataLoader(\n\u001b[1;32m---> 46\u001b[0m     \u001b[43mImageDataset\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata_dir\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mtrain\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtransforms\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtransforms_\u001b[49m\u001b[43m)\u001b[49m,\n\u001b[0;32m     47\u001b[0m     batch_size \u001b[38;5;241m=\u001b[39m batch_size,\n\u001b[0;32m     48\u001b[0m     shuffle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[0;32m     49\u001b[0m     num_workers \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m3\u001b[39m\n\u001b[0;32m     50\u001b[0m )\n\u001b[0;32m     52\u001b[0m validloader \u001b[38;5;241m=\u001b[39m DataLoader(\n\u001b[0;32m     53\u001b[0m     ImageDataset(data_dir, mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mvalid\u001b[39m\u001b[38;5;124m'\u001b[39m, transforms\u001b[38;5;241m=\u001b[39mtransforms_),\n\u001b[0;32m     54\u001b[0m     batch_size \u001b[38;5;241m=\u001b[39m batch_size,\n\u001b[0;32m     55\u001b[0m     shuffle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[0;32m     56\u001b[0m     num_workers \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m3\u001b[39m\n\u001b[0;32m     57\u001b[0m )\n",
            "Cell \u001b[1;32mIn[10], line 10\u001b[0m, in \u001b[0;36mImageDataset.__init__\u001b[1;34m(self, data_dir, mode, transforms)\u001b[0m\n\u001b[0;32m      7\u001b[0m B_dir \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(data_dir, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mVAE_generation_Cartoon/train\u001b[39m\u001b[38;5;124m'\u001b[39m)  \u001b[38;5;66;03m# modification forbidden\u001b[39;00m\n\u001b[0;32m      9\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m mode \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtrain\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[1;32m---> 10\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfiles_A \u001b[38;5;241m=\u001b[39m [os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(A_dir, name) \u001b[38;5;28;01mfor\u001b[39;00m name \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28msorted\u001b[39m(\u001b[43mos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlistdir\u001b[49m\u001b[43m(\u001b[49m\u001b[43mA_dir\u001b[49m\u001b[43m)\u001b[49m)[:\u001b[38;5;241m200\u001b[39m]] \u001b[38;5;66;03m# can be modified\u001b[39;00m\n\u001b[0;32m     11\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfiles_B \u001b[38;5;241m=\u001b[39m [os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(B_dir, name) \u001b[38;5;28;01mfor\u001b[39;00m name \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28msorted\u001b[39m(os\u001b[38;5;241m.\u001b[39mlistdir(B_dir))[:\u001b[38;5;241m200\u001b[39m]] \u001b[38;5;66;03m# can be modified\u001b[39;00m\n\u001b[0;32m     12\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m mode \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mvalid\u001b[39m\u001b[38;5;124m'\u001b[39m:\n",
            "\u001b[1;31mFileNotFoundError\u001b[0m: [WinError 3] The system cannot find the path specified: '/kaggle/input/group-project/image_image_translation\\\\VAE_generation/train'"
          ]
        }
      ],
      "source": [
        "\"\"\"\n",
        "Step 6. DataLoader\n",
        "\"\"\"\n",
        "class ImageDataset(Dataset):\n",
        "    def __init__(self, data_dir, mode='train', transforms=None, split_ratio=0.99):\n",
        "        A_dir = os.path.join(data_dir, 'VAE_generation/train') # modification forbidden\n",
        "        B_dir = os.path.join(data_dir, 'VAE_generation_Cartoon/train')  # modification forbidden\n",
        "\n",
        "        files_A = sorted(os.listdir(A_dir))\n",
        "        files_B = sorted(os.listdir(B_dir))\n",
        "\n",
        "        split_idx_A = int(len(files_A) * split_ratio)\n",
        "        split_idx_B = int(len(files_B) * split_ratio)\n",
        "\n",
        "        if mode == 'train':\n",
        "            self.files_A = [os.path.join(A_dir, name) for name in files_A[:split_idx_A]]\n",
        "            self.files_B = [os.path.join(B_dir, name) for name in files_B[:split_idx_B]]\n",
        "        elif mode == 'valid':\n",
        "            self.files_A = [os.path.join(A_dir, name) for name in files_A[split_idx_A:]]\n",
        "            self.files_B = [os.path.join(B_dir, name) for name in files_B[split_idx_B:]]\n",
        "\n",
        "        self.transforms = transforms\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.files_A)\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        file_A = self.files_A[index]\n",
        "        file_B = self.files_B[index]\n",
        "\n",
        "        img_A = Image.open(file_A)\n",
        "        img_B = Image.open(file_B)\n",
        "\n",
        "        if self.transforms is not None:\n",
        "            img_A = self.transforms(img_A)\n",
        "            img_B = self.transforms(img_B)\n",
        "\n",
        "        return img_A, img_B\n",
        "\n",
        "data_dir = './image_image_translation'\n",
        "# data_dir = '/kaggle/input/group-project/image_image_translation'\n",
        "\n",
        "image_size = (256, 256)\n",
        "data_transforms = transforms.Compose([\n",
        "    transforms.Resize(image_size),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
        "])\n",
        "\n",
        "face_transforms = get_face_specific_transforms(image_size)\n",
        "\n",
        "batch_size = 5\n",
        "\n",
        "trainloader = DataLoader(\n",
        "    ImageDataset(data_dir, mode='train', transforms=face_transforms),\n",
        "    batch_size = batch_size,\n",
        "    shuffle = True,\n",
        "    num_workers = 3\n",
        ")\n",
        "\n",
        "validloader = DataLoader(\n",
        "    ImageDataset(data_dir, mode='valid', transforms=data_transforms),\n",
        "    batch_size = batch_size,\n",
        "    shuffle = False,\n",
        "    num_workers = 3\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cfe1cb4d",
      "metadata": {},
      "outputs": [],
      "source": [
        "def weights_init_normal(m):\n",
        "    classname = m.__class__.__name__\n",
        "    if classname.find(\"Conv\") != -1:\n",
        "        torch.nn.init.normal_(m.weight.data, 0.0, 0.02)\n",
        "        if hasattr(m, \"bias\") and m.bias is not None:\n",
        "            torch.nn.init.constant_(m.bias.data, 0.0)\n",
        "    elif classname.find(\"BatchNorm2d\") != -1:\n",
        "        torch.nn.init.normal_(m.weight.data, 1.0, 0.02)\n",
        "        torch.nn.init.constant_(m.bias.data, 0.0)\n",
        "\n",
        "# Apply weight initialization\n",
        "G_AB.apply(weights_init_normal)\n",
        "G_BA.apply(weights_init_normal)\n",
        "D_A.apply(weights_init_normal)\n",
        "D_B.apply(weights_init_normal)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "X6b4-iUCyWAD",
      "metadata": {
        "id": "X6b4-iUCyWAD"
      },
      "source": [
        "## 4. Train your **model**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9p61i1E6yVQi",
      "metadata": {
        "id": "9p61i1E6yVQi"
      },
      "outputs": [],
      "source": [
        "\n",
        "def train(G_AB, G_BA, D_A, D_B, train_loader, validloader, num_epochs=200, batch_size=4):\n",
        "    # Setup optimizers\n",
        "    optimizer_G = torch.optim.Adam(\n",
        "        itertools.chain(G_AB.parameters(), G_BA.parameters()),\n",
        "        lr=lr, betas=betas\n",
        "    )\n",
        "    optimizer_D_A = torch.optim.Adam(D_A.parameters(), lr=lr, betas=betas)\n",
        "    optimizer_D_B = torch.optim.Adam(D_B.parameters(), lr=lr, betas=betas)\n",
        "    \n",
        "    # Setup LR schedulers\n",
        "    scheduler_G, scheduler_D_A, scheduler_D_B = get_lr_schedulers(\n",
        "        optimizer_G, optimizer_D_A, optimizer_D_B, num_epochs\n",
        "    )\n",
        "    \n",
        "    # Create image pools\n",
        "    fake_A_pool = ImagePool(pool_size_A)\n",
        "    fake_B_pool = ImagePool(pool_size_B)\n",
        "    \n",
        "    # Initialize adaptive loss weights\n",
        "    loss_weights = AdaptiveLossWeights({\n",
        "        'cycle': 10.0,\n",
        "        'identity': 5.0,\n",
        "        'gan': 1.0,\n",
        "        'frequency': 2.0,\n",
        "        'edge': 5.0,\n",
        "        'color': 1.0,\n",
        "        'feature_matching': 10.0\n",
        "    })\n",
        "    \n",
        "    # Loss histories for plotting\n",
        "    gen_loss_history = []\n",
        "    gen_AB_loss_history = []\n",
        "    gen_BA_loss_history = []\n",
        "    id_loss_history = []\n",
        "    gan_loss_history = []\n",
        "    cycle_loss_history = []\n",
        "    fm_loss_history = []\n",
        "    discA_loss_history = []\n",
        "    discB_loss_history = []\n",
        "    \n",
        "    # Training loop\n",
        "    for epoch in range(num_epochs):\n",
        "        # Initialize epoch losses\n",
        "        epoch_gen_losses = []\n",
        "        epoch_gen_AB_losses = []\n",
        "        epoch_gen_BA_losses = []\n",
        "        epoch_id_losses = []\n",
        "        epoch_gan_losses = []\n",
        "        epoch_cycle_losses = []\n",
        "        epoch_fm_losses = []\n",
        "        epoch_discA_losses = []\n",
        "        epoch_discB_losses = []\n",
        "        \n",
        "        for i, batch in enumerate(train_loader):\n",
        "            # Get batch data\n",
        "            real_A = batch['A'].type(Tensor)\n",
        "            real_B = batch['B'].type(Tensor)\n",
        "            \n",
        "            # Set model input\n",
        "            real_A = Variable(real_A)\n",
        "            real_B = Variable(real_B)\n",
        "            \n",
        "            # Adversarial ground truths\n",
        "            valid = Variable(Tensor(np.ones((real_A.size(0), 1))), requires_grad=False)\n",
        "            fake = Variable(Tensor(np.zeros((real_A.size(0), 1))), requires_grad=False)\n",
        "            \n",
        "            #-------------------------------\n",
        "            # Train Generators\n",
        "            #-------------------------------\n",
        "            optimizer_G.zero_grad()\n",
        "            \n",
        "            # Identity loss\n",
        "            identity_A = G_BA(real_A)\n",
        "            identity_B = G_AB(real_B)\n",
        "            loss_id_A = F.l1_loss(identity_A, real_A)\n",
        "            loss_id_B = F.l1_loss(identity_B, real_B)\n",
        "            loss_identity = (loss_id_A + loss_id_B) / 2\n",
        "            \n",
        "            # GAN loss\n",
        "            fake_B = G_AB(real_A)\n",
        "            fake_A = G_BA(real_B)\n",
        "            \n",
        "            # Get features for feature matching loss\n",
        "            real_A_pred, _, real_A_features = D_A(real_A, get_features=True)\n",
        "            fake_A_pred, _, fake_A_features = D_A(fake_A, get_features=True)\n",
        "            real_B_pred, _, real_B_features = D_B(real_B, get_features=True)\n",
        "            fake_B_pred, _, fake_B_features = D_B(fake_B, get_features=True)\n",
        "            \n",
        "            # Calculate GAN loss using relativistic loss\n",
        "            loss_GAN_AB = relativistic_loss((real_B_pred, _), (fake_B_pred, _), nn.MSELoss())\n",
        "            loss_GAN_BA = relativistic_loss((real_A_pred, _), (fake_A_pred, _), nn.MSELoss())\n",
        "            loss_GAN = (loss_GAN_AB + loss_GAN_BA) / 2\n",
        "            \n",
        "            # Cycle loss\n",
        "            recov_A = G_BA(fake_B)\n",
        "            recov_B = G_AB(fake_A)\n",
        "            loss_cycle_A = F.l1_loss(recov_A, real_A)\n",
        "            loss_cycle_B = F.l1_loss(recov_B, real_B)\n",
        "            loss_cycle = (loss_cycle_A + loss_cycle_B) / 2\n",
        "            \n",
        "            # Feature matching loss\n",
        "            loss_fm_A = feature_matching_loss(real_A_features, fake_A_features)\n",
        "            loss_fm_B = feature_matching_loss(real_B_features, fake_B_features)\n",
        "            loss_fm = (loss_fm_A + loss_fm_B) / 2\n",
        "            \n",
        "            # New losses\n",
        "            loss_freq = (fft_loss(real_A, recov_A) + fft_loss(real_B, recov_B)) / 2\n",
        "            loss_edge = (edge_loss(real_A, fake_A) + edge_loss(real_B, fake_B)) / 2\n",
        "            loss_color = color_histogram_loss(real_A, fake_A) + color_histogram_loss(real_B, fake_B)\n",
        "            \n",
        "            # Get current loss weights\n",
        "            weights = loss_weights.update({\n",
        "                'cycle': loss_cycle.item(),\n",
        "                'identity': loss_identity.item(),\n",
        "                'gan': loss_GAN.item(),\n",
        "                'frequency': loss_freq.item(),\n",
        "                'edge': loss_edge.item(),\n",
        "                'color': loss_color.item(),\n",
        "                'feature_matching': loss_fm.item()\n",
        "            })\n",
        "            \n",
        "            # Total generator loss with weighted components\n",
        "            loss_G = (\n",
        "                weights['gan'] * loss_GAN + \n",
        "                weights['cycle'] * loss_cycle +\n",
        "                weights['identity'] * loss_identity +\n",
        "                weights['frequency'] * loss_freq +\n",
        "                weights['edge'] * loss_edge +\n",
        "                weights['color'] * loss_color +\n",
        "                weights['feature_matching'] * loss_fm\n",
        "            )\n",
        "            \n",
        "            # Individual generator losses\n",
        "            loss_G_AB = (\n",
        "                weights['gan'] * loss_GAN_AB + \n",
        "                weights['cycle'] * loss_cycle_B +\n",
        "                weights['identity'] * loss_id_B +\n",
        "                weights['frequency'] * fft_loss(real_B, recov_B) +\n",
        "                weights['edge'] * edge_loss(real_B, fake_B) +\n",
        "                weights['color'] * color_histogram_loss(real_B, fake_B) +\n",
        "                weights['feature_matching'] * loss_fm_B\n",
        "            )\n",
        "            \n",
        "            loss_G_BA = (\n",
        "                weights['gan'] * loss_GAN_BA + \n",
        "                weights['cycle'] * loss_cycle_A +\n",
        "                weights['identity'] * loss_id_A +\n",
        "                weights['frequency'] * fft_loss(real_A, recov_A) +\n",
        "                weights['edge'] * edge_loss(real_A, fake_A) +\n",
        "                weights['color'] * color_histogram_loss(real_A, fake_A) +\n",
        "                weights['feature_matching'] * loss_fm_A\n",
        "            )\n",
        "            \n",
        "            loss_G.backward()\n",
        "            optimizer_G.step()\n",
        "            \n",
        "            #-------------------------------\n",
        "            # Train Discriminator A\n",
        "            #-------------------------------\n",
        "            optimizer_D_A.zero_grad()\n",
        "            \n",
        "            # Use image pool for discriminator training\n",
        "            fake_A_ = fake_A_pool.query(fake_A)\n",
        "            \n",
        "            # Real and fake discriminator outputs\n",
        "            real_A_pred, _ = D_A(real_A)\n",
        "            fake_A_pred, _ = D_A(fake_A_.detach())\n",
        "            \n",
        "            # Relativistic discriminator loss\n",
        "            loss_D_A = relativistic_loss((real_A_pred, _), (fake_A_pred, _), nn.MSELoss())\n",
        "            \n",
        "            # Add gradient penalty\n",
        "            loss_D_A_gp = compute_gradient_penalty(D_A, real_A, fake_A_.detach())\n",
        "            loss_D_A += 10.0 * loss_D_A_gp\n",
        "            \n",
        "            loss_D_A.backward()\n",
        "            optimizer_D_A.step()\n",
        "            \n",
        "            #-------------------------------\n",
        "            # Train Discriminator B\n",
        "            #-------------------------------\n",
        "            optimizer_D_B.zero_grad()\n",
        "            \n",
        "            # Use image pool for discriminator training\n",
        "            fake_B_ = fake_B_pool.query(fake_B)\n",
        "            \n",
        "            # Real and fake discriminator outputs\n",
        "            real_B_pred, _ = D_B(real_B)\n",
        "            fake_B_pred, _ = D_B(fake_B_.detach())\n",
        "            \n",
        "            # Relativistic discriminator loss\n",
        "            loss_D_B = relativistic_loss((real_B_pred, _), (fake_B_pred, _), nn.MSELoss())\n",
        "            \n",
        "            # Add gradient penalty\n",
        "            loss_D_B_gp = compute_gradient_penalty(D_B, real_B, fake_B_.detach())\n",
        "            loss_D_B += 10.0 * loss_D_B_gp\n",
        "            \n",
        "            loss_D_B.backward()\n",
        "            optimizer_D_B.step()\n",
        "            \n",
        "            # Store batch losses\n",
        "            epoch_gen_losses.append(loss_G.item())\n",
        "            epoch_gen_AB_losses.append(loss_G_AB.item())\n",
        "            epoch_gen_BA_losses.append(loss_G_BA.item())\n",
        "            epoch_id_losses.append(loss_identity.item())\n",
        "            epoch_gan_losses.append(loss_GAN.item())\n",
        "            epoch_cycle_losses.append(loss_cycle.item())\n",
        "            epoch_fm_losses.append(loss_fm.item())\n",
        "            epoch_discA_losses.append(loss_D_A.item())\n",
        "            epoch_discB_losses.append(loss_D_B.item())\n",
        "            \n",
        "            # Print progress\n",
        "            if i % 50 == 0:\n",
        "                print(f\"[Epoch {epoch}/{num_epochs}] [Batch {i}/{len(train_loader)}] \"\n",
        "                      f\"[D_A loss: {loss_D_A.item():.4f}] [D_B loss: {loss_D_B.item():.4f}] \"\n",
        "                      f\"[G loss: {loss_G.item():.4f}, adv: {loss_GAN.item():.4f}, cycle: {loss_cycle.item():.4f}, \"\n",
        "                      f\"id: {loss_identity.item():.4f}, fm: {loss_fm.item():.4f}]\")\n",
        "                \n",
        "        # Update learning rates\n",
        "        scheduler_G.step()\n",
        "        scheduler_D_A.step()\n",
        "        scheduler_D_B.step()\n",
        "        \n",
        "        # Calculate average epoch losses\n",
        "        gen_loss_history.append(sum(epoch_gen_losses) / len(epoch_gen_losses))\n",
        "        gen_AB_loss_history.append(sum(epoch_gen_AB_losses) / len(epoch_gen_AB_losses))\n",
        "        gen_BA_loss_history.append(sum(epoch_gen_BA_losses) / len(epoch_gen_BA_losses))\n",
        "        id_loss_history.append(sum(epoch_id_losses) / len(epoch_id_losses))\n",
        "        gan_loss_history.append(sum(epoch_gan_losses) / len(epoch_gan_losses))\n",
        "        cycle_loss_history.append(sum(epoch_cycle_losses) / len(epoch_cycle_losses))\n",
        "        fm_loss_history.append(sum(epoch_fm_losses) / len(epoch_fm_losses))\n",
        "        discA_loss_history.append(sum(epoch_discA_losses) / len(epoch_discA_losses))\n",
        "        discB_loss_history.append(sum(epoch_discB_losses) / len(epoch_discB_losses))\n",
        "        \n",
        "        # Print epoch losses\n",
        "        print_epoch_losses(epoch, epoch_gen_losses, epoch_gen_AB_losses, epoch_gen_BA_losses,\n",
        "                          epoch_id_losses, epoch_gan_losses, epoch_cycle_losses, epoch_fm_losses,\n",
        "                          epoch_discA_losses, epoch_discB_losses)\n",
        "        \n",
        "        if (epoch + 1) % 10 == 0:\n",
        "            # Set models to evaluation mode\n",
        "            G_AB.eval()\n",
        "            G_BA.eval()\n",
        "            \n",
        "            # Get validation samples\n",
        "            valid_iter = iter(validloader)\n",
        "            valid_real_A, valid_real_B = next(valid_iter)\n",
        "            valid_real_A, valid_real_B = valid_real_A.type(Tensor), valid_real_B.type(Tensor)\n",
        "            \n",
        "            # Generate validation results\n",
        "            with torch.no_grad():\n",
        "                # Generate fake samples\n",
        "                valid_fake_B = G_AB(valid_real_A)\n",
        "                valid_fake_A = G_BA(valid_real_B)\n",
        "                \n",
        "                # Generate reconstructed samples\n",
        "                valid_recov_A = G_BA(valid_fake_B)\n",
        "                valid_recov_B = G_AB(valid_fake_A)\n",
        "            \n",
        "            # Visualize validation results\n",
        "            show_images(valid_real_A, valid_fake_B, valid_recov_A,\n",
        "                    valid_real_B, valid_fake_A, valid_recov_B)\n",
        "    \n",
        "    plot_losses(gen_loss_history, gen_AB_loss_history, gen_BA_loss_history,\n",
        "           id_loss_history, gan_loss_history, cycle_loss_history, fm_loss_history,\n",
        "           discA_loss_history, discB_loss_history)\n",
        "    \n",
        "    save_checkpoint(G_AB, \"G_AB\", torch_seed)\n",
        "    save_checkpoint(G_BA, \"G_BA\", torch_seed)\n",
        "    save_checkpoint(D_A, \"D_A\", torch_seed)\n",
        "    save_checkpoint(D_B, \"D_B\", torch_seed)\n",
        "\n",
        "    return G_AB, G_BA, D_A, D_B, gen_loss_history"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3bbe635e",
      "metadata": {},
      "outputs": [],
      "source": [
        "G_AB, G_BA, D_A, D_B, loss_history = train(\n",
        "    G_AB, G_BA, D_A, D_B, \n",
        "    train_loader=trainloader, \n",
        "    validloader=validloader,\n",
        "    num_epochs=n_epochs, \n",
        "    batch_size=batch_size\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e66a8820",
      "metadata": {},
      "outputs": [],
      "source": [
        "def load_model(model, model_name):\n",
        "\n",
        "    # Load the Checkpoing\n",
        "    checkpoint = torch.load(f\"models/{file_name}-{model_name}-best.pth\")\n",
        "\n",
        "    # Load the state dict into the model\n",
        "    model.load_state_dict(checkpoint['model'])\n",
        "    torch_seed = checkpoint['torch_seed']\n",
        "\n",
        "    model.cuda()\n",
        "\n",
        "    return model, torch_seed"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "918f5e51",
      "metadata": {},
      "outputs": [],
      "source": [
        "G_AB, torch_seed = load_model(G_AB, \"G_AB\")\n",
        "G_BA, torch_seed = load_model(G_BA, \"G_BA\")\n",
        "D_A, torch_seed = load_model(D_A, \"D_A\")\n",
        "D_B, torch_seed = load_model(D_B, \"D_B\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "HIVgz01Kybct",
      "metadata": {
        "id": "HIVgz01Kybct"
      },
      "source": [
        "## 5. Evaluation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "BPlVhYZBxFFi",
      "metadata": {
        "id": "BPlVhYZBxFFi"
      },
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "Step 8. Generate Images\n",
        "\"\"\"\n",
        "## Translation 1: Raw Image --> Cartoon Image\n",
        "test_dir = os.path.join(data_dir, 'VAE_generation/test') # modification forbidden\n",
        "files = [os.path.join(test_dir, name) for name in os.listdir(test_dir)]\n",
        "\n",
        "save_dir = f'./generated_cartoon_images/{file_name}'\n",
        "if not os.path.exists(save_dir):\n",
        "    os.makedirs(save_dir)\n",
        "\n",
        "to_image = transforms.ToPILImage()\n",
        "\n",
        "G_AB.eval()\n",
        "for i in range(0, len(files), batch_size):\n",
        "    # read images\n",
        "    imgs = []\n",
        "    for j in range(i, min(len(files), i+batch_size)):\n",
        "        img = Image.open(files[j])\n",
        "        img = data_transforms(img)\n",
        "        imgs.append(img)\n",
        "    imgs = torch.stack(imgs, 0).type(Tensor)\n",
        "\n",
        "    # generate\n",
        "    fake_imgs = G_AB(imgs).detach().cpu()\n",
        "\n",
        "    # save\n",
        "    for j in range(fake_imgs.size(0)):\n",
        "        img = fake_imgs[j].squeeze().permute(1, 2, 0)\n",
        "        img_arr = img.numpy()\n",
        "        img_arr = (img_arr - np.min(img_arr)) * 255 / (np.max(img_arr) - np.min(img_arr))\n",
        "        img_arr = img_arr.astype(np.uint8)\n",
        "\n",
        "        img = to_image(img_arr)\n",
        "        _, name = os.path.split(files[i+j])\n",
        "        img.save(os.path.join(save_dir, name))\n",
        "\n",
        "gt_dir = os.path.join(data_dir, 'VAE_generation_Cartoon/test')\n",
        "metrics = torch_fidelity.calculate_metrics(\n",
        "    input1=save_dir,\n",
        "    input2=gt_dir,\n",
        "    cuda=True,\n",
        "    fid=True,\n",
        "    isc=True\n",
        ")\n",
        "\n",
        "fid_score = metrics[\"frechet_inception_distance\"]\n",
        "is_score = metrics[\"inception_score_mean\"]\n",
        "\n",
        "if is_score > 0:\n",
        "    s_value_1 = np.sqrt(fid_score / is_score)\n",
        "    print(\"Geometric Mean Score:\", s_value_1)\n",
        "else:\n",
        "    print(\"IS is 0, GMS cannot be computed!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bDJ3JMgSQBq2",
      "metadata": {
        "id": "bDJ3JMgSQBq2"
      },
      "outputs": [],
      "source": [
        "## Translation 2: Cartoon Image --> Raw Image\n",
        "test_dir = os.path.join(data_dir, 'VAE_generation_Cartoon/test') # modification forbidden\n",
        "files = [os.path.join(test_dir, name) for name in os.listdir(test_dir)]\n",
        "\n",
        "save_dir = f'./generated_raw_images/{file_name}'\n",
        "if not os.path.exists(save_dir):\n",
        "    os.makedirs(save_dir)\n",
        "\n",
        "G_BA.eval()\n",
        "for i in range(0, len(files), batch_size):\n",
        "    # read images\n",
        "    imgs = []\n",
        "    for j in range(i, min(len(files), i+batch_size)):\n",
        "        img = Image.open(files[j])\n",
        "        img = data_transforms(img)\n",
        "        imgs.append(img)\n",
        "    imgs = torch.stack(imgs, 0).type(Tensor)\n",
        "\n",
        "    # generate\n",
        "    fake_imgs = G_BA(imgs).detach().cpu()\n",
        "\n",
        "    # save\n",
        "    for j in range(fake_imgs.size(0)):\n",
        "        img = fake_imgs[j].squeeze().permute(1, 2, 0)\n",
        "        img_arr = img.numpy()\n",
        "        img_arr = (img_arr - np.min(img_arr)) * 255 / (np.max(img_arr) - np.min(img_arr))\n",
        "        img_arr = img_arr.astype(np.uint8)\n",
        "\n",
        "        img = to_image(img_arr)\n",
        "        _, name = os.path.split(files[i+j])\n",
        "        img.save(os.path.join(save_dir, name))\n",
        "\n",
        "gt_dir = os.path.join(data_dir, 'VAE_generation/test')\n",
        "\n",
        "metrics = torch_fidelity.calculate_metrics(\n",
        "    input1 = save_dir,\n",
        "    input2 = gt_dir,\n",
        "    cuda = True,\n",
        "    fid = True,\n",
        "    isc = True\n",
        ")\n",
        "\n",
        "fid_score = metrics[\"frechet_inception_distance\"]\n",
        "is_score = metrics[\"inception_score_mean\"]\n",
        "\n",
        "if is_score > 0:\n",
        "    s_value_2 = np.sqrt(fid_score / is_score)\n",
        "    print(\"Geometric Mean Score:\", s_value_2)\n",
        "else:\n",
        "    print(\"IS is 0, GMS cannot be computed!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "PSqBvUn5P-tA",
      "metadata": {
        "id": "PSqBvUn5P-tA"
      },
      "outputs": [],
      "source": [
        "s_value = np.round((s_value_1+s_value_2)/2, 5)\n",
        "print(s_value)\n",
        "df = pd.DataFrame({'id': [1], 'label': [s_value]})\n",
        "csv_path = \"zhirong.csv\"\n",
        "df.to_csv(csv_path, index=False)\n",
        "\n",
        "print(f\"CSV saved to {csv_path}\")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kaggle": {
      "accelerator": "gpu",
      "dataSources": [
        {
          "datasetId": 6717840,
          "sourceId": 10819976,
          "sourceType": "datasetVersion"
        }
      ],
      "dockerImageVersionId": 30887,
      "isGpuEnabled": true,
      "isInternetEnabled": true,
      "language": "python",
      "sourceType": "notebook"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.10"
    },
    "papermill": {
      "default_parameters": {},
      "duration": 9291.681683,
      "end_time": "2021-08-29T17:46:57.805996",
      "environment_variables": {},
      "exception": null,
      "input_path": "__notebook__.ipynb",
      "output_path": "__notebook__.ipynb",
      "parameters": {},
      "start_time": "2021-08-29T15:12:06.124313",
      "version": "2.3.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
