\documentclass[twoside,english,notitlepage]{report}

\usepackage{geometry}
\usepackage{lipsum}
\usepackage[T1]{fontenc}
\usepackage{babel}
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{cite}
\usepackage{hyperref}
\usepackage{mathbbol}

\geometry{
    a4paper,
    total={170mm,257mm},
    left=20mm,
    top=10mm,
}

\makeatletter
\def\@makechapterhead#1{%
    \vspace*{0\p@}%                                 % Insert 50pt (vertical) space
    {\parindent \z@ \raggedright \normalfont         % No paragraph indent, ragged right
    \ifnum \c@secnumdepth >\m@ne                   % If you should number chapters
        % \if@mainmatter                               % ... and you're in \mainmatter
        % \huge\bfseries \@chapapp\space \thechapter % huge, bold, Chapter + number – this is original
        \Huge\bfseries \thechapter.\space%
        % \par\nobreak                               % paragraph break without page break
        % \vskip 20\p@                               % Insert 20pt (vertical) space
    \fi
    \interlinepenalty\@M                           % Penalty
    \Huge \bfseries #1\par\nobreak                 % Huge, bold chapter title
    \vskip 30\p@                                   % Insert 40pt (vertical) space
}}
\makeatother


\title{CS424 – Group Project Report}
\author{
    Akeela Darryl Fattha\\
    \texttt{akeelaf.2022@scis.smu.edu.sg}
    \and
    Fadhel Erlangga Wibawanto\\
    \texttt{fadhelew.2022@scis.smu.edu.sg}
    \and
    Tan Zhi Rong\\
    \texttt{zhirong.tan.2022@scis.smu.edu.sg}
    \and
    Grace Angel Bisawan \\
    \texttt{gbisawan.2022@scis.smu.edu.sg}
    \and
    Lee Jia Heng\\
    \texttt{jiaheng.lee.2023@scis.smu.edu.sg}
}

\begin{document}
\date{}
\maketitle
\begin{abstract}
This report provides an overview of Cycle-Consistent Adversarial Networks (CycleGAN), a technique for unpaired image-to-image translation. We explore the architecture, key innovations, applications, and limitations of CycleGAN models in computer vision tasks.
\end{abstract}
\tableofcontents


\chapter{Task 1}

\section{Introduction}
Image-to-image translation is the task of converting an image from one domain to another. CycleGAN, introduced by Zhu et al. in 2017, addresses the challenge of learning such translations without paired training data. This makes it particularly useful for applications where paired examples are difficult or impossible to obtain.

\subsection{Configuration}





\section{Architecture}
\subsection{Discriminator}
\paragraph{} Our discriminator uses a basic sequential architecture with convolutional layers that doubles in number of channels 4 times, from 64 to 512. Each convolution output is passed to a spectral norm layer and a leaky relu activation function. 

\paragraph{} We also made use of a PatchGAN discriminator, accumulate whether these patches can be used to determine if the image is real or fake.

\subsection{Generator}
\paragraph{} Our generator incorporates several modern attention mechanisms and architectural techniques including Convolutional Block Attention Module (CBAM), Squeeze-and-Excitation (SE) blocks, and Self-Attention mechanisms for improved feature representation, generating higher-quality images with better detail preservation, style consistency, and structural coherence.

\paragraph{} Some general key design choices include:
\begin{itemize}
    \item \textbf{Instance Normalization:} Used throughout the network instead of batch normalization, as it has been shown to produce better results for style transfer and image-to-image translation by normalizing each instance independently.

    \item \textbf{Reflection Padding:} Applied before convolutions to reduce boundary artifacts that can appear in generated images, particularly important for maintaining realistic edges.

    \item \textbf{Mixed Residual Blocks:} The strategic placement of different residual block types allows the network to benefit from complementary approaches to feature transformation:
    \begin{itemize}
        \item Style-modulated blocks provide explicit control over stylistic elements
        \item CBAM-enhanced blocks in the middle layers help focus on important features
        \item All blocks benefit from SE attention for channel recalibration
    \end{itemize}
    
    \item \textbf{Squeeze-and-Excitation Attention:} Used in Mixed Residual Blocks, it recalibrates channel-wise feature responses by explicitly modeling interdependencies between channels, allowing the network to selectively emphasize informative features.

    \item \textbf{Enhanced Self-Attention:} Positioned after the residual blocks, it helps ensure global coherence in the generated images by modeling long-range dependencies that convolutional operations cannot capture efficiently.

\end{itemize}

\paragraph{ImprovedResidualBlock:} These blocks enhance the standard residual connection with both SE attention and conditional CBAM:

CBAM is applied to blocks 3-6, providing dual attention mechanisms:
\begin{itemize}
    \item \textbf{Channel Attention:} Models interdependencies between channels using both max and average pooling operations.
    \item \textbf{Spatial Attention:} Focuses on important spatial regions by creating attention maps from channel-wise statistics.
\end{itemize}

\paragraph{StyleModulatedResBlock:} Every third residual block employs style modulation through Adaptive Instance Normalization:
\begin{align}
    \text{AdaIN}(x) = \gamma \cdot \frac{x - \mu(x)}{\sigma(x)} + \beta
\end{align}

where $\gamma$ and $\beta$ are learnable style scale and bias parameters. This allows the network to better control stylistic elements in the generated images.

\subsubsection{Enhanced Self-Attention}
After the residual blocks, an enhanced self-attention module is applied:
\begin{align}
    \text{Attention}(Q, K, V) = \text{Softmax}\left(\frac{QK^T + P}{\sqrt{d_k}}\right)V
\end{align}

where $P$ represents the positional bias that emphasizes the central region. This self-attention mechanism allows the network to model long-range dependencies and coherence in the image, which is particularly valuable for maintaining structural integrity in facial regions.


\section{Data Preparation}
\subsection{Augmentations}
\paragraph{} We started with resizing images to 128x128, paired with a batch size of 16 before we hit a memory limit. We also applied no augmentations to the images, as a baseline to compare against later on when we add augmentations.

\paragraph{} After adding various minor augmentations like affines, color jitters, flips, light gaussian blurs and posterising, we found that any combination of them resulted in worse metrics overall. From visual inspection, we found that the generated raw images were just slightly more "noisy" than without augmentations, which could contribute to the worse metrics. Furthermore, the generated cartoon images were a lot more "flat" looking than the target images, losing a lot more of the finer details and textures. 

\paragraph{} Thus, in both cases, we felt that adding the augmentations were encouraging the model to over-process the style conversions, which likely caused the reduction in metrics. 

\subsection{Training/Validation}
A simple 80/20 split on training data with validation



\section{Loss Functions}
Our overall loss functions for CycleGAN are as follows:


\begin{equation}
    L_{total} = L_{GAN} + \lambda_{cyc} L_{cyc} + \lambda_{id} L_{id} + \lambda_{edge} L_{edge} + \lambda_{color} L_{color}
\end{equation}

\subsection{Discriminator}
\subsubsection{Patch}
We use a PatchGAN discriminator, which classifies whether 70x70 overlapping image patches are real or fake. This allows the model to focus on local image features and helps in generating high-quality images.

\subsubsection{Adversarial}
We use a basic BCE loss as well bruh.

\subsection{Generator}

\subsubsection{Adversarial}
Use basic BCE loss to compare between predicted \& real predictions. 

\subsubsection{Cycle Consistency}
Basic L1 loss to ensure that it tries to get back to where it started exactly. 

\subsubsection{Identity}
We use a combination of MSE and Edge Consistency loss to determine our identity loss. We put additional weight on Edge Consistency as it is extremely important to maintain the edges of the image, especially when it comes to maintaining the structure of a "face". However, to ensure that the model keeps within the color distribution \& looks of the original image, we include MSE loss as well. 

\subsubsection{Edge Consistency}
As mentioned, this loss is used to determine whether the structure of the images are maintained. Firstly, we grayscaled our images as color is not our priority for this loss computation. Next, we use sobel kernels to determine the edges of the images, which will be passed to a structural similariy computation function by Piqa. 

\subsubsection{Color Consistency}


\subsection{Adaptive Loss Weighting}
We also incorporate adaptive loss weighting to alter the compositions of the different loss functions according to how far along the training we are. During early training, it is more important to focus on the cycle consistency loss, as the model is still learning the basic structure of the images. As training progresses, we shift the focus towards the adversarial loss and identity loss to refine the generated images and ensure they closely resemble the target domain.






\chapter{Task 2}
\section{Introduction}



\subsection{Configuration}



\section{Architecture}
\subsection{Discriminator}
GANs consist of two networks: a generator that creates images and a discriminator that evaluates them. The two networks are trained adversarially, with the generator trying to fool the discriminator.

\subsection{Generator}
CycleGAN extends the GAN framework by using two generator-discriminator pairs, allowing translation between domains X and Y. The key innovation is the cycle-consistency loss, which ensures that translating an image to the target domain and back produces the original image.


\section{Data Preparation}
\subsection{Pre-Processing}


\subsection{Training/Validation}
A simple 80/20 split on training data with validation



\section{Loss Functions}
Our overall loss functions for CycleGAN are as follows:


\begin{equation}
    L_{total} = L_{GAN} + \lambda_{cyc} L_{cyc} + \lambda_{id} L_{id} + \lambda_{edge} L_{edge} + \lambda_{color} L_{color}
\end{equation}

\subsection{Discriminator}
\subsubsection{Patch}

\subsubsection{Least Squares}

\subsection{Generator}

\subsubsection{Adversarial}
Use basic 

\subsubsection{Cycle Consistency}

\subsubsection{Identity}

\subsubsection{Edge Consistency}

\subsubsection{Color Consistency}

\subsection{Adaptive Loss Weighting}







\bibliographystyle{plain}
\bibliography{references}  % Create a references.bib file with your citations

\end{document}