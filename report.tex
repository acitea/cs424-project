\documentclass[twoside,english,notitlepage]{report}

\usepackage{geometry}
\usepackage{lipsum}
\usepackage[T1]{fontenc}
\usepackage{babel}
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{cite}
\usepackage{hyperref}
\usepackage{mathbbol}
\usepackage{multirow}
\usepackage{adjustbox}
\usepackage{multicol}
\usepackage[outermargin=-4cm,]{fullwidth}

\geometry{
    a4paper,
    total={170mm,257mm},
    left=20mm,
    top=10mm,
}

\makeatletter
\def\@makechapterhead#1{%
    \vspace*{0\p@}%                                 % Insert 50pt (vertical) space
    {\parindent \z@ \raggedright \normalfont         % No paragraph indent, ragged right
    \ifnum \c@secnumdepth >\m@ne                   % If you should number chapters
        % \if@mainmatter                               % ... and you're in \mainmatter
        % \huge\bfseries \@chapapp\space \thechapter % huge, bold, Chapter + number – this is original
        \Huge\bfseries \thechapter.\space%
        % \par\nobreak                               % paragraph break without page break
        % \vskip 20\p@                               % Insert 20pt (vertical) space
    \fi
    \interlinepenalty\@M                           % Penalty
    \Huge \bfseries #1\par\nobreak                 % Huge, bold chapter title
    \vskip 30\p@                                   % Insert 40pt (vertical) space
}}
\makeatother


\title{CS424 – Group Project Report}
\author{
    Akeela Darryl Fattha\\
    \texttt{akeelaf.2022@scis.smu.edu.sg}
    \and
    Fadhel Erlangga Wibawanto\\
    \texttt{fadhelew.2022@scis.smu.edu.sg}
    \and
    Tan Zhi Rong\\
    \texttt{zhirong.tan.2022@scis.smu.edu.sg}
    \and
    Grace Angel Bisawan \\
    \texttt{gbisawan.2022@scis.smu.edu.sg}
    \and
    Lee Jia Heng\\
    \texttt{jiaheng.lee.2023@scis.smu.edu.sg}
}

\begin{document}
\date{}
\maketitle
\begin{abstract}
This report provides an overview of Cycle-Consistent Adversarial Networks (CycleGAN), a technique for unpaired image-to-image translation. We explore the architecture, key innovations, applications, and limitations of CycleGAN models in computer vision tasks.
\end{abstract}
\tableofcontents


\chapter{Task 1}

\section{Introduction}
We attempt to create a CyleGAN using novel and modern techniques to improve the performance of the model to convert cartoon faces to realistic faces, and vice-versa. We will be using improved blocks, various attention mechanisms and normalisation techniques in our generators, as well as additional loss functions to better guide the models to learn the mapping between the two domains. We opted not to use any augmentations to our images, and we use an image size of 256x256 as our input/output size. \\\\
With our enhancements, we achieved the following results:

\begin{table}[h]
    \centering
    \begin{tabular}{|l|c|c|c|c|}
    \hline
    \textbf{} & \textbf{FID} & \textbf{IS} & \textbf{GMS} & \textbf{Avg GMS} \\
    \cline{1-5}
    Raw to Cartoon     & 46.70193     & 2.41466 ± 0.24392   & 4.39783    & \multirow{2}{*}{3.85666} \\
    Cartoon to Raw     & 39.83461         & 3.62382 ± 0.23992        & 3.31548        &            \\
    \hline
    \end{tabular}
\end{table}

\subsection{Configuration \& Hyperparameters}
All optimisers used Adam with a Cosine Annealing Warm Restarts scheduler. The following hyperparameters and loss weights were used for our best results:

\begin{multicols}{3}
    \begin{itemize}
        \item Random seed: 42
        \item Torch seed: 1722426635407800
        \item Epochs: 100
        \item Batch size: 3
        \item Learning rate: 2e-4
        \item Betas: (0.5, 0.999)
        \item $Scheduler_{t-0}$: 10
        \item $Scheduler_{t-mult}$: 2
        \item $Scheduler_{min-lr}$: 1e-5
        \item $\lambda_{cyc}$: 10.0
        \item $\lambda_{id}$: 5.0
        \item $\lambda_{gan}$: 1.0
        \item $\lambda_{gp}$: 10.0
        \item $\lambda_{fm}$: 5.0
    \end{itemize}
\end{multicols}

\section{Architecture}

\subsection{Generator}\label{task1:generator}
Our generator incorporates several modern attention mechanisms and architectural techniques including Convolutional Block Attention Module (CBAM), Squeeze-and-Excitation (SE) blocks, and Self-Attention mechanisms for improved feature representation, generating higher-quality images with better detail preservation, style consistency, and structural coherence. Some of our key design choices are listed below:

\begin{itemize}
    \item \textbf{Instance Normalization:} Used throughout the network instead of batch normalization, as it has been shown to produce better results for style transfer and image-to-image translation by normalizing each instance independently.

    \item \textbf{Reflection Padding:} Applied before convolutions to reduce boundary artifacts that can appear in generated images, particularly important for maintaining realistic edges.

    \item \textbf{Mixed + Improved Residual Blocks:} The strategic placement of different residual block types allows the network to benefit from complementary approaches to feature transformation. They also exhibit the following characteristics and sub-blocks:
    \begin{itemize}
        \item \textbf{Channel Attention:} Models interdependencies between channels using both max and average pooling operations.
        \item \textbf{Spatial Attention:} Focuses on important spatial regions by creating attention maps from channel-wise statistics.v
        \item All blocks benefit from SE attention for channel recalibration
    \end{itemize}

    \item \textbf{Squeeze-and-Excitation Attention:} Used in Mixed Residual Blocks, it recalibrates channel-wise feature responses by explicitly modeling interdependencies between channels, allowing the network to selectively emphasize informative features.

    \item \textbf{Enhanced Self-Attention:} Positioned after the residual blocks, it helps ensure global coherence in the generated images by modeling long-range dependencies that convolutional operations cannot capture efficiently. This is particularly valuable for maintaining structural integrity in facial regions.

\end{itemize}

\begin{figure}[h]
    \centering
    \includegraphics[width=\linewidth]{task1/generator-architecture-h.png}
    \caption{Generator Architecture}
    \begin{fullwidth}[innermargin=-2cm,width=\linewidth+4cm,nobreak]
        \begin{minipage}[t]{.5\textwidth}
            \centering
            \adjustbox{valign=t}{\includegraphics[scale=0.5]{task1/self-attn.jpg}}
            \\Self Attention Block
        \end{minipage}
        \begin{minipage}[t]{.5\textwidth}
            \centering
            \adjustbox{valign=t}{\includegraphics[scale=0.31]{task1/cbam.jpg}}
        \end{minipage}
    \end{fullwidth}
\end{figure}

% \newpage
\subsection{Discriminator}\label{task1:discriminator}
Our discriminator uses a basic sequential architecture with convolutional layers that doubles in number of channels 3 times, from 64 to 512. Each convolution output is passed to a spectral norm layer, instance normalisation and a leaky relu activation function. We then branched into a Avg Pool into a FC layer for global classification and a PatchGAN block for local classification. \\

Given that we have used a 256x256 image, the feature map will have size of $[B, 512, 16, 16]$, the patchGAN will have size of $[B, 1, 16, 16]$. A larger patch GAN and feature map allows us to capture more details of specific features in certain areas which allows the discriminator to provide more useful information to the generator.


\begin{figure}[h]
    \centering
    \includegraphics[width=0.7\textwidth]{task1/discriminator-architecture.pdf}
    \caption{Discriminator Architecture}
\end{figure}


\section{Data Preparation}
\subsection{Augmentations}
We started with resizing images to 128x128, with a batch size of 16 before we hit a memory limit. We also applied no augmentations to the images, as a baseline to compare against later on when we add augmentations. \\

\noindent After adding various minor augmentations like affines, color jitters, flips, light gaussian blurs and posterising, we found that any combination of them resulted in worse metrics overall. From visual inspection, we found that the generated raw images were more noisy than without augmentations, which could contribute to the worse metrics. Furthermore, the generated cartoon images were a lot more flat than the target images, losing a lot more of the finer details and textures. Lastly, affines and any crops were clearly visible in the generated images when they otherwise should not be. \\

\noindent Thus, in both cases, we believe that adding the augmentations were encouraging the model to over-process the style conversions, which likely caused the reduction in metrics. Finally our best result was achieved with no augmentations, and upscaling the images to 256x256 with a batch size of 3. Some sample results are shown in the below Figures.

\newpage
\begin{figure}[h]
    \centering
    \includegraphics[width=0.8\linewidth]{task1/originals.jpg}
    \vspace{-10pt}
    \caption{Original images. Top row: Raw, Bottom row: Cartoon}
    \vspace{12pt}
    \includegraphics[width=0.8\linewidth]{task1/top2.jpg}
    \vspace{-10pt}
    \caption{128x128 images Avg GMS: 4.47829}
    \vspace{12pt}
    \includegraphics[width=0.8\linewidth]{task1/top1.jpg}
    \vspace{-10pt}
    \caption{256x256 images Avg GMS: 3.85666}
\end{figure}

\newpage
\subsection{Training/Validation}
We worked with simple 99/1 split on training/validation data since we found that we needed as much data as possible for training the model to achieve our best results. \\

\noindent We also made use of an "Image Pool" to help in training. This pool functions as a buffer to store previously generated images, which helps to reduce the risk of mode collapse where the Discriminator becomes too powerful, and prevents the Generator from learning anything new. The pool is updated with new generated fake images during training, and once the threshold is reached, we randomly sample from the available pool instead of using the latest generated image. 

\section{Loss Functions}

\subsection{Discriminator Losses}
We used a combination of loss functions beyond the standard adversarial loss to improve the performance of our discriminator. Our overall loss function for the discriminator is as follows:
\begin{equation}
    L_{D} = L_{GAN} + \lambda_{gp} L_{gp}
\end{equation}

\subsubsection{Gradient Penalty – $L_{gp}$}
We also use a gradient penalty, inspired from Wasserstein GAN (WGAN)\footnote{Arjovsky, M., Chintala, S., \& Bottou, L. (2017, July 17). Wasserstein Generative Adversarial networks. PMLR. \href{https://proceedings.mlr.press/v70/arjovsky17a.html}{https://proceedings.mlr.press/v70/arjovsky17a.html}} in order to prevent the discriminator from learning too fast. Doing so avoids problems like mode collapse with vanishing gradients, and ensures that the generator is always learning, even if it is not performing well enough to "beat" the discriminator yet, thus improving stability of training.

\subsubsection{Relativistic – $L_{GAN}$}
We extend the typical adversarial loss by using a relativistic loss, which compares the logits of discriminating the real and fake images in a relative manner on a global scope with Binary Cross Entropy Loss. Furthermore, we took inspiration from PatchGAN\footnote{Isola, P., Zhu, J., Zhou, T., \& Efros, A. A. (2016). Image-to-Image Translation with Conditional Adversarial Networks. arXiv (Cornell University). \href{https://doi.org/10.48550/arxiv.1611.07004}{https://doi.org/10.48550/arxiv.1611.07004}} which whether the images is real or fake based on smaller patches, a local scope. Thus, we have another source of information for determining real or fake, which is computed with MSE loss. The equation for our relativistic loss for our "real" side is as follows, and the opposite will be applied for the "fake" side i.e. $D_{real} <=> D_{fake}, True <=> False$:
\begin{equation}
    L_{GAN} = 0.7 \cdot MSE(D_{real} - \bar{D}_{fake}, True)_{patch} + 0.3 \cdot BCE(D_{real} - \bar{D}_{fake}, True)_{global}
\end{equation}

\subsection{Generator Losses}
The generator also makes use of the \textbf{Relativistic} loss, which is an extension of the standard adversarial GAN loss. Our overall loss function for the generator is as follows:
\begin{equation}
    L_{G} = L_{GAN} + \lambda_{cyc} L_{cyc} + \lambda_{id} L_{id} + \lambda_{feat} L_{feat}
\end{equation}
\subsubsection{Cycle Consistency – $L_{cyc}$}
As we would like the fake images to be able revertible to its original image i.e. the pixels are identical, we maintain using L1 Loss for our Cycle Consistency.

\subsubsection{Identity – $L_{id}$}
Similarly, we also use L1 Loss for our Identity loss since images in the same domain should not be altered. 
\subsubsection{Feature – $L_{feat}$}
Another extension that we incorporated is the Feature Loss, which serves as a perceptual loss that compares the features of the generated image with the target image. Although traditionally used with pre-trained networks, we used the features extracted from the training discriminator instead. These features are compared using L1 loss, which helps to ensure that the generated images are perceptually similar to the target images. 


\begin{figure}[h]
    \centering
    \includegraphics[width=\textwidth]{task1/loss_curves.jpg}
    \caption{Overall loss curves}
\end{figure}

\chapter{Task 2}
\section{Introduction}



\subsection{Configuration}



\section{Architecture}
\subsection{Generator}
We used a similar architecture as the one used in Task 1 (\ref{task1:generator}), with the following modification specific to catering for vast difference in domains.

\textbf{Vector Quantization Block} is used to convert the input image into a discrete representation, which is then passed through a series of residual blocks. The output of the residual blocks is then passed through a decoder to reconstruct the image. The vector quantization block is used to learn a set of discrete codes that can be used to represent the input image, allowing for more efficient encoding and decoding. 

which will learn additional features, as animal and pokemon do not have a direct translation.


\begin{figure}[h]
    \centering
    \includegraphics[width=\textwidth]{task2/vq-block.jpg}
    \caption{Vector Quantization Block}
\end{figure}



\subsection{Discriminator}
We used the same general architecture as the one used in Task 1 (\ref{task1:discriminator}), with a few modifications specific to catering for the smaller images. 

Since our Pokémon sprite sizes are only 96x96, we removed the last downsampling layer of the discriminator so bottleneck/features will be capped at $[B, 256, 12, 12]$. The reasons are two-fold: 

\begin{itemize}
    \item The structure of an animal and pokemon being very different, so we will not want to make the discriminator too powerful. 
    \item A $[B, 256, 6, 6]$ feature map will not provide enough useful information to the generator either, thus a $[B, 256, 12, 12]$ feature map can better help the generator to identify the Pokémon sprite style.
\end{itemize}



\section{Data Preparation}
\subsection{Collection}
Due to our task of transfering the styles from a real-life animal to that of a Pokémon sprite, we were unable to find a paired dataset between these 2 domains. Thus, we resorted to utilising 2 individual datasets: one for the animal images, another for the Pokémon sprites, and then combining them together.

\subsubsection{Animal Dataset}
The animal dataset \footnote{Animal Image Dataset (90 Different Animals) – Zooming in on Wildlife: 5400 Animal Images Across 90 Diverse Classes \href{https://www.kaggle.com/datasets/iamsouravbanerjee/animal-image-dataset-90-different-animals}{https://www.kaggle.com/datasets/iamsouravbanerjee/animal-image-dataset-90-different-animals}} we used consists of 5400 images of animals from 90 different classes / types of animals. We decided to use all images from the animal dataset. (Usability: 10.0 Total Image: 5.4k)



\subsubsection{Pokémon Sprite Dataset}
We decided to use Pokémon sprites as comapared to the original Pokémon art as we there is a distinct style to how Pokémon sprites are. Furthermore, using the original Pokémon art would not be suitable for a "style transfer" as we felt that the original art is at the prerogative of the individual artists when they created them. Thus, the styles across Pokémon will not be consistent, and our model will find it very difficult to learn the mapping between the two domains. Our failed attempts can be found in the Appendix \ref{task2:poke-official-fail} \\


The Pokémon sprite dataset \footnote{Pokemon sprite images – Total 10,437 Pokemon sprite images in 96x96 resolution. \\ \href{https://www.kaggle.com/datasets/yehongjiang/pokemon-sprites-images}{https://www.kaggle.com/datasets/yehongjiang/pokemon-sprites-images}} we used consists of many different variations of the same pokemon, namely those front facing, back facing, shiny and normal. We have decided to use the normal and front facing versions as those are the most suitable given that our animal datasets also exhibit the same characteristics. (Usability 5.29, Total Image: 10.2k)

\subsection{Pre-Processing}
\subsubsection{Background Removal}
As we realize that most of the pokemon sprites have a white background, we decided to use a pretrained model (VGG16) to remove the background of the animal and convert them into white background.

This will help the model to focus on the animal -> pokemon transformation rather than trying to convert the background to white.


\subsubsection{No Paired Images}
We decided to randomly pair each animal to a random pokemon and we ensure that each pokemon is not used too many times, as some pokemon have many images while some only have a few. After pairing, we have a total of 5400 animal-pokemon paired images. 


We only have 1025 unique pokemon, with different number of front facing pokemons, with 5400 different animal images, we try to ensure that each type of pokemon sprite is equally represented (at most 6 times for the pokemon to be used)

\begin{figure}[h]
    \centering
    \includegraphics[width=\textwidth]{task2/poke-fail2.jpg}
    \vspace{-10pt}
    \caption{Failed pairing}
\end{figure}


Used ChatGPT to generate a list of mapping of animal to pokemon variations


used a pretrained vgg16 to remove background for animals



\subsection{Training/Validation}
A simple 90/10 train-test split. So, we will have 4860 train images and 540 test images. 
The train (4860 images) is split further to a 99/1 split, where 4811 images will be used for training, while 49 will be used for validation (visualisation of how the pokemon will look like)



\section{Loss Functions}
Our overall loss functions for CycleGAN are as follows:


\begin{equation}
    L_{total} = L_{GAN} + \lambda_{cyc} L_{cyc} + \lambda_{id} L_{id} + \lambda_{edge} L_{edge} + \lambda_{color} L_{color}
\end{equation}

\subsection{Discriminator}
\subsubsection{Patch}

\subsubsection{Least Squares}

\subsection{Generator}

\subsubsection{Adversarial}
Use basic 

\subsubsection{Cycle Consistency}

\subsubsection{Identity}

\subsubsection{Edge Consistency}

\subsubsection{Color Consistency}

\subsection{Adaptive Loss Weighting}



\section{Appendix}

\subsubsection{Pokémon Official Art}\label{task2:poke-official-fail}
Other dataset tried https://www.kaggle.com/datasets/hesselaar/all-pokemon-official-images
This data consisted of 1010 official pokemon art. However, after training, the results seems to be really bad.


\begin{figure}[h]
    \centering
    \includegraphics[width=\textwidth]{task2/poke-fail1.jpg}
    \vspace{-10pt}
    \caption{Random sample 1}\label{fig:poke-fail1}
\end{figure}

Notice that in \ref{fig:poke-fail1} the generated image does not resemble Pokémon art at all. We suspect that it is trying to colour the animal images in but it does now know exactly in what colour to use. We extensively researched on the reasons why and got cooked.





\bibliographystyle{plain}
\bibliography{references}  % Create a references.bib file with your citations

\end{document}