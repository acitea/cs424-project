\documentclass[twoside,english,notitlepage]{report}

\usepackage{geometry}
\usepackage{lipsum}
\usepackage[T1]{fontenc}
\usepackage{babel}
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{cite}
\usepackage{hyperref}
\usepackage{mathbbol}

\geometry{
    a4paper,
    total={170mm,257mm},
    left=20mm,
    top=10mm,
}

\makeatletter
\def\@makechapterhead#1{%
    \vspace*{0\p@}%                                 % Insert 50pt (vertical) space
    {\parindent \z@ \raggedright \normalfont         % No paragraph indent, ragged right
    \ifnum \c@secnumdepth >\m@ne                   % If you should number chapters
        % \if@mainmatter                               % ... and you're in \mainmatter
        % \huge\bfseries \@chapapp\space \thechapter % huge, bold, Chapter + number – this is original
        \Huge\bfseries \thechapter.\space%
        % \par\nobreak                               % paragraph break without page break
        % \vskip 20\p@                               % Insert 20pt (vertical) space
    \fi
    \interlinepenalty\@M                           % Penalty
    \Huge \bfseries #1\par\nobreak                 % Huge, bold chapter title
    \vskip 30\p@                                   % Insert 40pt (vertical) space
}}
\makeatother


\title{CS424 – Group Project Report}
\author{
    Akeela Darryl Fattha\\
    \texttt{akeelaf.2022@scis.smu.edu.sg}
    \and
    Fadhel Erlangga Wibawanto\\
    \texttt{fadhelew.2022@scis.smu.edu.sg}
    \and
    Tan Zhi Rong\\
    \texttt{zhirong.tan.2022@scis.smu.edu.sg}
    \and
    Grace Angel Bisawan \\
    \texttt{gbisawan.2022@scis.smu.edu.sg}
    \and
    Lee Jia Heng\\
    \texttt{jiaheng.lee.2023@scis.smu.edu.sg}
}

\begin{document}
\date{}
\maketitle
\begin{abstract}
This report provides an overview of Cycle-Consistent Adversarial Networks (CycleGAN), a technique for unpaired image-to-image translation. We explore the architecture, key innovations, applications, and limitations of CycleGAN models in computer vision tasks.
\end{abstract}
\tableofcontents


\chapter{Task 1}

\section{Introduction}
Image-to-image translation is the task of converting an image from one domain to another. CycleGAN, introduced by Zhu et al. in 2017, addresses the challenge of learning such translations without paired training data. This makes it particularly useful for applications where paired examples are difficult or impossible to obtain.

\subsection{Configuration}





\section{Architecture}
\subsection{Discriminator}
\paragraph{} Our discriminator uses a basic sequential architecture with convolutional layers that doubles in number of channels 4 times, from 64 to 512. Each convolution output is passed to a spectral norm layer and a leaky relu activation function. 

\paragraph{} We also made use of a PatchGAN discriminator, accumulate whether these patches can be used to determine if the image is real or fake.

inspired by Pix2Pix, patchGAN true/false, global true/false, and feature output fed into generator. 


\subsection{Generator}
\paragraph{} Our generator incorporates several modern attention mechanisms and architectural techniques including Convolutional Block Attention Module (CBAM), Squeeze-and-Excitation (SE) blocks, and Self-Attention mechanisms for improved feature representation, generating higher-quality images with better detail preservation, style consistency, and structural coherence.

\paragraph{} Some general key design choices include:
\begin{itemize}
    \item \textbf{Instance Normalization:} Used throughout the network instead of batch normalization, as it has been shown to produce better results for style transfer and image-to-image translation by normalizing each instance independently.

    \item \textbf{Reflection Padding:} Applied before convolutions to reduce boundary artifacts that can appear in generated images, particularly important for maintaining realistic edges.

    \item \textbf{Mixed Residual Blocks:} The strategic placement of different residual block types allows the network to benefit from complementary approaches to feature transformation:

    \begin{itemize}
        \item Style-modulated blocks provide explicit control over stylistic elements
        \item CBAM-enhanced blocks in the middle layers help focus on important features
        \item All blocks benefit from SE attention for channel recalibration
    \end{itemize}

    \item \textbf{Squeeze-and-Excitation Attention:} Used in Mixed Residual Blocks, it recalibrates channel-wise feature responses by explicitly modeling interdependencies between channels, allowing the network to selectively emphasize informative features.

    \item \textbf{Enhanced Self-Attention:} Positioned after the residual blocks, it helps ensure global coherence in the generated images by modeling long-range dependencies that convolutional operations cannot capture efficiently.

\end{itemize}

\paragraph{ImprovedResidualBlock:} These blocks enhance the standard residual connection with both SE attention and conditional CBAM:

CBAM is applied to blocks 3-6, providing dual attention mechanisms:
\begin{itemize}
    \item \textbf{Channel Attention:} Models interdependencies between channels using both max and average pooling operations.
    \item \textbf{Spatial Attention:} Focuses on important spatial regions by creating attention maps from channel-wise statistics.
\end{itemize}

\paragraph{StyleModulatedResBlock:} Every third residual block employs style modulation through Adaptive Instance Normalization:
\begin{align}
    \text{AdaIN}(x) = \gamma \cdot \frac{x - \mu(x)}{\sigma(x)} + \beta
\end{align}

where $\gamma$ and $\beta$ are learnable style scale and bias parameters. This allows the network to better control stylistic elements in the generated images.

\subsubsection{Enhanced Self-Attention}
After the residual blocks, an enhanced self-attention module is applied:
\begin{align}
    \text{Attention}(Q, K, V) = \text{Softmax}\left(\frac{QK^T + P}{\sqrt{d_k}}\right)V
\end{align}

where $P$ represents the positional bias that emphasizes the central region. This self-attention mechanism allows the network to model long-range dependencies and coherence in the image, which is particularly valuable for maintaining structural integrity in facial regions.


\section{Data Preparation}
\subsection{Augmentations}
\paragraph{} We started with resizing images to 128x128, paired with a batch size of 16 before we hit a memory limit. We also applied no augmentations to the images, as a baseline to compare against later on when we add augmentations. 

\paragraph{} After adding various minor augmentations like affines, color jitters, flips, light gaussian blurs and posterising, we found that any combination of them resulted in worse metrics overall. From visual inspection, we found that the generated raw images were more noisy than without augmentations, which could contribute to the worse metrics. Furthermore, the generated cartoon images were a lot more flat than the target images, losing a lot more of the finer details and textures. Lastly, affines and any crops were clearly visible in the generated images when they otherwise should not be. 

\paragraph{} Thus, in both cases, we felt that adding the augmentations were encouraging the model to over-process the style conversions, which likely caused the reduction in metrics. 

\subsection{Training/Validation}
\paragraph{} We worked with simple 99/1 split on training/validation data since we found that we needed as much data as possible for training the model.

\paragraph{} We also made use of an "Image Pool" to help in training. This pool functions as a buffer to store previously generated images, which helps to reduce the risk of mode collapse where the Discriminator becomes too powerful, and prevents the Generator from learning anything new. The pool is updated with new generated fake images during training, and once the threshold is reached, we randomly sample from the available pool instead of using the latest generated image. 

\section{Loss Functions}
Our overall loss functions for CycleGAN are as follows:


\begin{equation}
    L_{total} = L_{GAN} + \lambda_{cyc} L_{cyc} + \lambda_{id} L_{id} + \lambda_{edge} L_{edge} + \lambda_{color} L_{color}
\end{equation}

\subsection{Discriminator}

\subsubsection{Gradient Penalty}
We also use a gradient penalty, inspired from Wasserstein GAN (WGAN)\footnotetext{Arjovsky, M., Chintala, S., & Bottou, L. (2017, July 17). Wasserstein Generative Adversarial networks. PMLR. \href{https://proceedings.mlr.press/v70/arjovsky17a.html}{https://proceedings.mlr.press/v70/arjovsky17a.html}} which improves stability of training, avoids problems like mode collapse with vanishing gradients, and ensures that the generator is always learning, even if it is not performing well enough to "beat" the discriminator. 

\subsubsection{Relativistic}
We extend the typical adversarial loss by using a relativistic loss, which compares the logits of discriminating the real and fake images in a relative manner with Binary Cross Entropy Loss. Furthermore, we took inspiration from PatchGAN

Use basic BCE loss to compare between predicted \& real predictions, on top of MSE for the Patch  

\subsection{Generator}
\subsubsection{Cycle Consistency}
Basic L1 loss to ensure that it tries to get back to where it started exactly. 

\subsubsection{Identity}
We use a combination of MSE and Edge Consistency loss to determine our identity loss. We put additional weight on Edge Consistency as it is extremely important to maintain the edges of the image, especially when it comes to maintaining the structure of a "face". However, to ensure that the model keeps within the color distribution \& looks of the original image, we include MSE loss as well. 

\subsubsection{Feature}
MSE over the feature map of the Discriminator input



\chapter{Task 2}
\section{Introduction}



\subsection{Configuration}



\section{Architecture}
\subsection{Discriminator}
GANs consist of two networks: a generator that creates images and a discriminator that evaluates them. The two networks are trained adversarially, with the generator trying to fool the discriminator.

\subsection{Generator}
CycleGAN extends the GAN framework by using two generator-discriminator pairs, allowing translation between domains X and Y. The key innovation is the cycle-consistency loss, which ensures that translating an image to the target domain and back produces the original image.


\section{Data Preparation}
\subsection{Pre-Processing}
Used ChatGPT to generate a list of mapping of animal to pokemon variations


used a pretrained vgg16 to remove background for animals



\subsection{Training/Validation}
A simple 80/20 split on training data with validation



\section{Loss Functions}
Our overall loss functions for CycleGAN are as follows:


\begin{equation}
    L_{total} = L_{GAN} + \lambda_{cyc} L_{cyc} + \lambda_{id} L_{id} + \lambda_{edge} L_{edge} + \lambda_{color} L_{color}
\end{equation}

\subsection{Discriminator}
\subsubsection{Patch}

\subsubsection{Least Squares}

\subsection{Generator}

\subsubsection{Adversarial}
Use basic 

\subsubsection{Cycle Consistency}

\subsubsection{Identity}

\subsubsection{Edge Consistency}

\subsubsection{Color Consistency}

\subsection{Adaptive Loss Weighting}







\bibliographystyle{plain}
\bibliography{references}  % Create a references.bib file with your citations

\end{document}